{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# ! pip install split-folders\n",
    "import splitfolders\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rs = 42\n",
    "def reset_random_seeds(rs):\n",
    "   os.environ['PYTHONHASHSEED']=str(rs)\n",
    "   tf.random.set_seed(rs)\n",
    "   np.random.seed(rs)\n",
    "   random.seed(rs)\n",
    "reset_random_seeds(rs)\n",
    "\n",
    "map_dir = 'map 5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove prev saved database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './data/humanModel_dataset_split'\n",
    "if os.path.exists(dir_path):\n",
    "    shutil.rmtree(dir_path)\n",
    "else:\n",
    "    print(\"File not found in the directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train , test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 5834 files [00:09, 610.52 files/s]\n"
     ]
    }
   ],
   "source": [
    "# train, test split\n",
    "splitfolders.ratio('./data/humanModel_dataset/', output=\"./data/humanModel_dataset_split\", ratio=(0.8, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4664 images belonging to 5 classes.\n",
      "Found 1170 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "IMG_SIZE = 10\n",
    "\n",
    "datagen = ImageDataGenerator(rescale = 1./255)\n",
    "train = datagen.flow_from_directory('./data/humanModel_dataset_split/train',\n",
    "                                          target_size = (IMG_SIZE,IMG_SIZE), batch_size = batch_size, seed=rs, class_mode='sparse')\n",
    "\n",
    "test = datagen.flow_from_directory('./data/humanModel_dataset_split/val',\n",
    "                                            target_size = (IMG_SIZE, IMG_SIZE), batch_size = batch_size, seed=rs, class_mode='sparse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "\n",
    "model = Sequential([\n",
    "  layers.Conv2D(8, 4, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=[IMG_SIZE,IMG_SIZE,3]),\n",
    "  layers.Conv2D(16, 4, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(8, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 10, 10, 8)         392       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 10, 10, 16)        2064      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 16)        2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 5, 8)           1160      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 5, 5, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                6432      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 12,533\n",
      "Trainable params: 12,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\keras\\backend.py:4907: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  '\"`sparse_categorical_crossentropy` received `from_logits=True`, but '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - ETA: 8:44 - loss: 1.7127 - accuracy: 0.12 - ETA: 10s - loss: 1.7083 - accuracy: 0.1576 - ETA: 4s - loss: 1.7025 - accuracy: 0.210 - ETA: 3s - loss: 1.6988 - accuracy: 0.23 - ETA: 2s - loss: 1.6950 - accuracy: 0.24 - ETA: 2s - loss: 1.6934 - accuracy: 0.25 - ETA: 2s - loss: 1.6907 - accuracy: 0.25 - ETA: 1s - loss: 1.6869 - accuracy: 0.25 - ETA: 1s - loss: 1.6816 - accuracy: 0.26 - ETA: 1s - loss: 1.6737 - accuracy: 0.27 - ETA: 1s - loss: 1.6687 - accuracy: 0.28 - ETA: 1s - loss: 1.6669 - accuracy: 0.28 - ETA: 1s - loss: 1.6640 - accuracy: 0.28 - ETA: 1s - loss: 1.6573 - accuracy: 0.29 - ETA: 0s - loss: 1.6551 - accuracy: 0.29 - ETA: 0s - loss: 1.6495 - accuracy: 0.29 - ETA: 0s - loss: 1.6455 - accuracy: 0.29 - ETA: 0s - loss: 1.6432 - accuracy: 0.30 - ETA: 0s - loss: 1.6357 - accuracy: 0.30 - ETA: 0s - loss: 1.6306 - accuracy: 0.31 - ETA: 0s - loss: 1.6257 - accuracy: 0.31 - ETA: 0s - loss: 1.6227 - accuracy: 0.31 - ETA: 0s - loss: 1.6149 - accuracy: 0.32 - ETA: 0s - loss: 1.6079 - accuracy: 0.33 - ETA: 0s - loss: 1.6017 - accuracy: 0.33 - ETA: 0s - loss: 1.5952 - accuracy: 0.34 - ETA: 0s - loss: 1.5882 - accuracy: 0.35 - 10s 43ms/step - loss: 1.5882 - accuracy: 0.3516 - val_loss: 1.3221 - val_accuracy: 0.5385\n",
      "Epoch 2/200\n",
      "73/73 [==============================] - ETA: 6s - loss: 1.5101 - accuracy: 0.46 - ETA: 2s - loss: 1.4434 - accuracy: 0.46 - ETA: 2s - loss: 1.3988 - accuracy: 0.50 - ETA: 2s - loss: 1.3612 - accuracy: 0.53 - ETA: 2s - loss: 1.3372 - accuracy: 0.54 - ETA: 1s - loss: 1.3336 - accuracy: 0.54 - ETA: 1s - loss: 1.3177 - accuracy: 0.54 - ETA: 1s - loss: 1.3066 - accuracy: 0.55 - ETA: 1s - loss: 1.3063 - accuracy: 0.55 - ETA: 1s - loss: 1.2989 - accuracy: 0.55 - ETA: 1s - loss: 1.2887 - accuracy: 0.55 - ETA: 1s - loss: 1.2934 - accuracy: 0.55 - ETA: 0s - loss: 1.2997 - accuracy: 0.54 - ETA: 0s - loss: 1.2879 - accuracy: 0.54 - ETA: 0s - loss: 1.2800 - accuracy: 0.54 - ETA: 0s - loss: 1.2750 - accuracy: 0.54 - ETA: 0s - loss: 1.2698 - accuracy: 0.54 - ETA: 0s - loss: 1.2682 - accuracy: 0.54 - ETA: 0s - loss: 1.2659 - accuracy: 0.54 - ETA: 0s - loss: 1.2616 - accuracy: 0.55 - ETA: 0s - loss: 1.2606 - accuracy: 0.55 - ETA: 0s - loss: 1.2616 - accuracy: 0.55 - ETA: 0s - loss: 1.2578 - accuracy: 0.55 - ETA: 0s - loss: 1.2556 - accuracy: 0.55 - ETA: 0s - loss: 1.2545 - accuracy: 0.54 - ETA: 0s - loss: 1.2535 - accuracy: 0.54 - 3s 34ms/step - loss: 1.2515 - accuracy: 0.5497 - val_loss: 1.0998 - val_accuracy: 0.5949\n",
      "Epoch 3/200\n",
      "73/73 [==============================] - ETA: 5s - loss: 1.1650 - accuracy: 0.53 - ETA: 2s - loss: 1.0782 - accuracy: 0.57 - ETA: 2s - loss: 1.0494 - accuracy: 0.59 - ETA: 2s - loss: 1.0881 - accuracy: 0.57 - ETA: 1s - loss: 1.1323 - accuracy: 0.56 - ETA: 1s - loss: 1.1432 - accuracy: 0.56 - ETA: 1s - loss: 1.1294 - accuracy: 0.57 - ETA: 1s - loss: 1.1261 - accuracy: 0.58 - ETA: 1s - loss: 1.1379 - accuracy: 0.57 - ETA: 1s - loss: 1.1463 - accuracy: 0.57 - ETA: 1s - loss: 1.1404 - accuracy: 0.58 - ETA: 1s - loss: 1.1377 - accuracy: 0.58 - ETA: 0s - loss: 1.1321 - accuracy: 0.58 - ETA: 0s - loss: 1.1333 - accuracy: 0.58 - ETA: 0s - loss: 1.1280 - accuracy: 0.59 - ETA: 0s - loss: 1.1320 - accuracy: 0.59 - ETA: 0s - loss: 1.1325 - accuracy: 0.59 - ETA: 0s - loss: 1.1370 - accuracy: 0.59 - ETA: 0s - loss: 1.1395 - accuracy: 0.59 - ETA: 0s - loss: 1.1334 - accuracy: 0.59 - ETA: 0s - loss: 1.1316 - accuracy: 0.59 - ETA: 0s - loss: 1.1297 - accuracy: 0.59 - ETA: 0s - loss: 1.1311 - accuracy: 0.59 - ETA: 0s - loss: 1.1300 - accuracy: 0.59 - ETA: 0s - loss: 1.1295 - accuracy: 0.59 - ETA: 0s - loss: 1.1293 - accuracy: 0.59 - ETA: 0s - loss: 1.1252 - accuracy: 0.59 - 3s 35ms/step - loss: 1.1252 - accuracy: 0.5980 - val_loss: 0.9806 - val_accuracy: 0.6444\n",
      "Epoch 4/200\n",
      "73/73 [==============================] - ETA: 6s - loss: 1.0865 - accuracy: 0.64 - ETA: 2s - loss: 1.1135 - accuracy: 0.59 - ETA: 2s - loss: 1.0160 - accuracy: 0.64 - ETA: 2s - loss: 1.0231 - accuracy: 0.62 - ETA: 2s - loss: 1.0351 - accuracy: 0.61 - ETA: 1s - loss: 1.0284 - accuracy: 0.61 - ETA: 1s - loss: 1.0511 - accuracy: 0.60 - ETA: 1s - loss: 1.0422 - accuracy: 0.61 - ETA: 1s - loss: 1.0405 - accuracy: 0.61 - ETA: 1s - loss: 1.0369 - accuracy: 0.61 - ETA: 1s - loss: 1.0309 - accuracy: 0.62 - ETA: 1s - loss: 1.0312 - accuracy: 0.62 - ETA: 1s - loss: 1.0271 - accuracy: 0.62 - ETA: 1s - loss: 1.0344 - accuracy: 0.62 - ETA: 0s - loss: 1.0455 - accuracy: 0.62 - ETA: 0s - loss: 1.0402 - accuracy: 0.62 - ETA: 0s - loss: 1.0390 - accuracy: 0.62 - ETA: 0s - loss: 1.0365 - accuracy: 0.63 - ETA: 0s - loss: 1.0377 - accuracy: 0.63 - ETA: 0s - loss: 1.0452 - accuracy: 0.63 - ETA: 0s - loss: 1.0466 - accuracy: 0.62 - ETA: 0s - loss: 1.0462 - accuracy: 0.63 - ETA: 0s - loss: 1.0433 - accuracy: 0.63 - ETA: 0s - loss: 1.0403 - accuracy: 0.63 - ETA: 0s - loss: 1.0373 - accuracy: 0.63 - ETA: 0s - loss: 1.0384 - accuracy: 0.64 - ETA: 0s - loss: 1.0425 - accuracy: 0.64 - ETA: 0s - loss: 1.0415 - accuracy: 0.64 - 3s 35ms/step - loss: 1.0415 - accuracy: 0.6422 - val_loss: 0.9085 - val_accuracy: 0.7291\n",
      "Epoch 5/200\n",
      "73/73 [==============================] - ETA: 5s - loss: 1.0661 - accuracy: 0.64 - ETA: 2s - loss: 0.9949 - accuracy: 0.64 - ETA: 2s - loss: 0.9407 - accuracy: 0.66 - ETA: 2s - loss: 0.9414 - accuracy: 0.66 - ETA: 2s - loss: 0.9832 - accuracy: 0.66 - ETA: 1s - loss: 0.9823 - accuracy: 0.67 - ETA: 1s - loss: 0.9871 - accuracy: 0.67 - ETA: 1s - loss: 0.9899 - accuracy: 0.67 - ETA: 1s - loss: 1.0025 - accuracy: 0.66 - ETA: 1s - loss: 1.0080 - accuracy: 0.66 - ETA: 1s - loss: 1.0103 - accuracy: 0.66 - ETA: 1s - loss: 1.0065 - accuracy: 0.66 - ETA: 1s - loss: 1.0046 - accuracy: 0.65 - ETA: 0s - loss: 1.0036 - accuracy: 0.65 - ETA: 0s - loss: 0.9922 - accuracy: 0.66 - ETA: 0s - loss: 0.9884 - accuracy: 0.66 - ETA: 0s - loss: 0.9921 - accuracy: 0.66 - ETA: 0s - loss: 0.9886 - accuracy: 0.66 - ETA: 0s - loss: 0.9920 - accuracy: 0.66 - ETA: 0s - loss: 0.9862 - accuracy: 0.67 - ETA: 0s - loss: 0.9879 - accuracy: 0.66 - ETA: 0s - loss: 0.9919 - accuracy: 0.66 - ETA: 0s - loss: 0.9861 - accuracy: 0.66 - ETA: 0s - loss: 0.9875 - accuracy: 0.66 - ETA: 0s - loss: 0.9805 - accuracy: 0.67 - ETA: 0s - loss: 0.9740 - accuracy: 0.67 - ETA: 0s - loss: 0.9767 - accuracy: 0.67 - 3s 34ms/step - loss: 0.9751 - accuracy: 0.6717 - val_loss: 0.8585 - val_accuracy: 0.7308\n",
      "Epoch 6/200\n",
      "73/73 [==============================] - ETA: 6s - loss: 1.0791 - accuracy: 0.60 - ETA: 2s - loss: 1.1420 - accuracy: 0.58 - ETA: 2s - loss: 1.1227 - accuracy: 0.62 - ETA: 1s - loss: 1.0024 - accuracy: 0.66 - ETA: 1s - loss: 0.9826 - accuracy: 0.67 - ETA: 1s - loss: 0.9795 - accuracy: 0.67 - ETA: 1s - loss: 0.9795 - accuracy: 0.67 - ETA: 1s - loss: 0.9505 - accuracy: 0.69 - ETA: 1s - loss: 0.9603 - accuracy: 0.68 - ETA: 1s - loss: 0.9646 - accuracy: 0.68 - ETA: 1s - loss: 0.9650 - accuracy: 0.68 - ETA: 1s - loss: 0.9590 - accuracy: 0.68 - ETA: 0s - loss: 0.9571 - accuracy: 0.68 - ETA: 0s - loss: 0.9554 - accuracy: 0.68 - ETA: 0s - loss: 0.9543 - accuracy: 0.68 - ETA: 0s - loss: 0.9523 - accuracy: 0.68 - ETA: 0s - loss: 0.9472 - accuracy: 0.68 - ETA: 0s - loss: 0.9451 - accuracy: 0.68 - ETA: 0s - loss: 0.9521 - accuracy: 0.68 - ETA: 0s - loss: 0.9537 - accuracy: 0.68 - ETA: 0s - loss: 0.9530 - accuracy: 0.68 - ETA: 0s - loss: 0.9508 - accuracy: 0.68 - ETA: 0s - loss: 0.9492 - accuracy: 0.68 - ETA: 0s - loss: 0.9484 - accuracy: 0.68 - ETA: 0s - loss: 0.9477 - accuracy: 0.68 - ETA: 0s - loss: 0.9451 - accuracy: 0.68 - 2s 33ms/step - loss: 0.9436 - accuracy: 0.6850 - val_loss: 0.8328 - val_accuracy: 0.7393\n",
      "Epoch 7/200\n",
      "73/73 [==============================] - ETA: 6s - loss: 0.9179 - accuracy: 0.67 - ETA: 2s - loss: 0.8989 - accuracy: 0.67 - ETA: 2s - loss: 0.8924 - accuracy: 0.68 - ETA: 1s - loss: 0.9126 - accuracy: 0.67 - ETA: 1s - loss: 0.9065 - accuracy: 0.68 - ETA: 1s - loss: 0.9300 - accuracy: 0.66 - ETA: 1s - loss: 0.9280 - accuracy: 0.67 - ETA: 1s - loss: 0.9270 - accuracy: 0.67 - ETA: 1s - loss: 0.9271 - accuracy: 0.67 - ETA: 1s - loss: 0.9169 - accuracy: 0.68 - ETA: 1s - loss: 0.9127 - accuracy: 0.68 - ETA: 1s - loss: 0.9206 - accuracy: 0.68 - ETA: 1s - loss: 0.9259 - accuracy: 0.68 - ETA: 1s - loss: 0.9308 - accuracy: 0.68 - ETA: 1s - loss: 0.9306 - accuracy: 0.67 - ETA: 0s - loss: 0.9319 - accuracy: 0.67 - ETA: 0s - loss: 0.9334 - accuracy: 0.68 - ETA: 0s - loss: 0.9306 - accuracy: 0.68 - ETA: 0s - loss: 0.9253 - accuracy: 0.68 - ETA: 0s - loss: 0.9272 - accuracy: 0.68 - ETA: 0s - loss: 0.9267 - accuracy: 0.68 - ETA: 0s - loss: 0.9317 - accuracy: 0.68 - ETA: 0s - loss: 0.9324 - accuracy: 0.68 - ETA: 0s - loss: 0.9325 - accuracy: 0.68 - ETA: 0s - loss: 0.9312 - accuracy: 0.68 - ETA: 0s - loss: 0.9295 - accuracy: 0.68 - ETA: 0s - loss: 0.9278 - accuracy: 0.68 - ETA: 0s - loss: 0.9324 - accuracy: 0.68 - ETA: 0s - loss: 0.9347 - accuracy: 0.68 - ETA: 0s - loss: 0.9324 - accuracy: 0.68 - ETA: 0s - loss: 0.9271 - accuracy: 0.68 - ETA: 0s - loss: 0.9220 - accuracy: 0.68 - ETA: 0s - loss: 0.9226 - accuracy: 0.68 - ETA: 0s - loss: 0.9234 - accuracy: 0.68 - 3s 40ms/step - loss: 0.9210 - accuracy: 0.6904 - val_loss: 0.8052 - val_accuracy: 0.7316\n",
      "Epoch 8/200\n",
      "73/73 [==============================] - ETA: 7s - loss: 1.0216 - accuracy: 0.65 - ETA: 2s - loss: 0.9115 - accuracy: 0.68 - ETA: 2s - loss: 0.9252 - accuracy: 0.68 - ETA: 1s - loss: 0.9087 - accuracy: 0.68 - ETA: 1s - loss: 0.8972 - accuracy: 0.68 - ETA: 1s - loss: 0.9015 - accuracy: 0.68 - ETA: 1s - loss: 0.8990 - accuracy: 0.68 - ETA: 1s - loss: 0.8976 - accuracy: 0.68 - ETA: 1s - loss: 0.9018 - accuracy: 0.68 - ETA: 1s - loss: 0.9031 - accuracy: 0.68 - ETA: 1s - loss: 0.9055 - accuracy: 0.68 - ETA: 1s - loss: 0.9001 - accuracy: 0.68 - ETA: 0s - loss: 0.8936 - accuracy: 0.68 - ETA: 0s - loss: 0.8909 - accuracy: 0.69 - ETA: 0s - loss: 0.8969 - accuracy: 0.69 - ETA: 0s - loss: 0.8989 - accuracy: 0.69 - ETA: 0s - loss: 0.8934 - accuracy: 0.69 - ETA: 0s - loss: 0.8908 - accuracy: 0.69 - ETA: 0s - loss: 0.8909 - accuracy: 0.69 - ETA: 0s - loss: 0.8914 - accuracy: 0.69 - ETA: 0s - loss: 0.8861 - accuracy: 0.69 - ETA: 0s - loss: 0.8958 - accuracy: 0.69 - ETA: 0s - loss: 0.8974 - accuracy: 0.69 - ETA: 0s - loss: 0.8958 - accuracy: 0.69 - ETA: 0s - loss: 0.8969 - accuracy: 0.69 - ETA: 0s - loss: 0.8935 - accuracy: 0.69 - 2s 33ms/step - loss: 0.8966 - accuracy: 0.6951 - val_loss: 0.7838 - val_accuracy: 0.7590\n",
      "Epoch 9/200\n",
      "73/73 [==============================] - ETA: 6s - loss: 0.9837 - accuracy: 0.76 - ETA: 2s - loss: 0.9379 - accuracy: 0.70 - ETA: 2s - loss: 0.9131 - accuracy: 0.71 - ETA: 2s - loss: 0.8772 - accuracy: 0.71 - ETA: 2s - loss: 0.8930 - accuracy: 0.70 - ETA: 1s - loss: 0.9038 - accuracy: 0.70 - ETA: 1s - loss: 0.8795 - accuracy: 0.71 - ETA: 1s - loss: 0.8891 - accuracy: 0.70 - ETA: 1s - loss: 0.8794 - accuracy: 0.71 - ETA: 1s - loss: 0.9014 - accuracy: 0.70 - ETA: 1s - loss: 0.8893 - accuracy: 0.70 - ETA: 1s - loss: 0.8774 - accuracy: 0.71 - ETA: 1s - loss: 0.8841 - accuracy: 0.71 - ETA: 0s - loss: 0.8839 - accuracy: 0.71 - ETA: 0s - loss: 0.8751 - accuracy: 0.71 - ETA: 0s - loss: 0.8748 - accuracy: 0.71 - ETA: 0s - loss: 0.8728 - accuracy: 0.71 - ETA: 0s - loss: 0.8668 - accuracy: 0.71 - ETA: 0s - loss: 0.8672 - accuracy: 0.71 - ETA: 0s - loss: 0.8681 - accuracy: 0.71 - ETA: 0s - loss: 0.8662 - accuracy: 0.71 - ETA: 0s - loss: 0.8708 - accuracy: 0.71 - ETA: 0s - loss: 0.8724 - accuracy: 0.71 - ETA: 0s - loss: 0.8722 - accuracy: 0.71 - ETA: 0s - loss: 0.8708 - accuracy: 0.71 - ETA: 0s - loss: 0.8733 - accuracy: 0.71 - ETA: 0s - loss: 0.8762 - accuracy: 0.71 - 3s 35ms/step - loss: 0.8790 - accuracy: 0.7105 - val_loss: 0.8029 - val_accuracy: 0.7573\n",
      "Epoch 10/200\n",
      "73/73 [==============================] - ETA: 6s - loss: 0.7665 - accuracy: 0.70 - ETA: 2s - loss: 0.8794 - accuracy: 0.70 - ETA: 2s - loss: 0.8473 - accuracy: 0.71 - ETA: 2s - loss: 0.8605 - accuracy: 0.72 - ETA: 2s - loss: 0.9249 - accuracy: 0.69 - ETA: 1s - loss: 0.9307 - accuracy: 0.70 - ETA: 1s - loss: 0.9031 - accuracy: 0.71 - ETA: 1s - loss: 0.9075 - accuracy: 0.70 - ETA: 1s - loss: 0.9113 - accuracy: 0.69 - ETA: 1s - loss: 0.9075 - accuracy: 0.70 - ETA: 1s - loss: 0.9077 - accuracy: 0.70 - ETA: 1s - loss: 0.8939 - accuracy: 0.71 - ETA: 0s - loss: 0.8959 - accuracy: 0.70 - ETA: 0s - loss: 0.8940 - accuracy: 0.70 - ETA: 0s - loss: 0.8914 - accuracy: 0.70 - ETA: 0s - loss: 0.8931 - accuracy: 0.70 - ETA: 0s - loss: 0.8848 - accuracy: 0.70 - ETA: 0s - loss: 0.8785 - accuracy: 0.71 - ETA: 0s - loss: 0.8767 - accuracy: 0.71 - ETA: 0s - loss: 0.8727 - accuracy: 0.71 - ETA: 0s - loss: 0.8722 - accuracy: 0.71 - ETA: 0s - loss: 0.8710 - accuracy: 0.71 - ETA: 0s - loss: 0.8709 - accuracy: 0.71 - ETA: 0s - loss: 0.8674 - accuracy: 0.71 - ETA: 0s - loss: 0.8649 - accuracy: 0.71 - ETA: 0s - loss: 0.8645 - accuracy: 0.71 - 3s 34ms/step - loss: 0.8626 - accuracy: 0.7168 - val_loss: 0.7465 - val_accuracy: 0.7761\n",
      "Epoch 11/200\n",
      "73/73 [==============================] - ETA: 6s - loss: 0.7851 - accuracy: 0.73 - ETA: 2s - loss: 0.7541 - accuracy: 0.77 - ETA: 2s - loss: 0.7586 - accuracy: 0.75 - ETA: 2s - loss: 0.7708 - accuracy: 0.74 - ETA: 1s - loss: 0.7735 - accuracy: 0.74 - ETA: 1s - loss: 0.8027 - accuracy: 0.74 - ETA: 1s - loss: 0.8083 - accuracy: 0.73 - ETA: 1s - loss: 0.8047 - accuracy: 0.73 - ETA: 1s - loss: 0.8092 - accuracy: 0.73 - ETA: 1s - loss: 0.8095 - accuracy: 0.73 - ETA: 1s - loss: 0.8000 - accuracy: 0.74 - ETA: 1s - loss: 0.8015 - accuracy: 0.74 - ETA: 0s - loss: 0.8167 - accuracy: 0.73 - ETA: 0s - loss: 0.8302 - accuracy: 0.73 - ETA: 0s - loss: 0.8324 - accuracy: 0.73 - ETA: 0s - loss: 0.8275 - accuracy: 0.73 - ETA: 0s - loss: 0.8279 - accuracy: 0.73 - ETA: 0s - loss: 0.8339 - accuracy: 0.73 - ETA: 0s - loss: 0.8309 - accuracy: 0.73 - ETA: 0s - loss: 0.8316 - accuracy: 0.73 - ETA: 0s - loss: 0.8328 - accuracy: 0.73 - ETA: 0s - loss: 0.8315 - accuracy: 0.73 - ETA: 0s - loss: 0.8289 - accuracy: 0.73 - ETA: 0s - loss: 0.8328 - accuracy: 0.73 - ETA: 0s - loss: 0.8337 - accuracy: 0.73 - ETA: 0s - loss: 0.8355 - accuracy: 0.73 - ETA: 0s - loss: 0.8349 - accuracy: 0.73 - 3s 34ms/step - loss: 0.8349 - accuracy: 0.7318 - val_loss: 0.7312 - val_accuracy: 0.7752\n",
      "Epoch 12/200\n",
      "73/73 [==============================] - ETA: 7s - loss: 0.8200 - accuracy: 0.73 - ETA: 2s - loss: 0.9424 - accuracy: 0.67 - ETA: 2s - loss: 0.8863 - accuracy: 0.71 - ETA: 2s - loss: 0.8689 - accuracy: 0.71 - ETA: 1s - loss: 0.8439 - accuracy: 0.73 - ETA: 1s - loss: 0.8573 - accuracy: 0.71 - ETA: 1s - loss: 0.8503 - accuracy: 0.72 - ETA: 1s - loss: 0.8384 - accuracy: 0.72 - ETA: 1s - loss: 0.8487 - accuracy: 0.72 - ETA: 1s - loss: 0.8504 - accuracy: 0.72 - ETA: 1s - loss: 0.8443 - accuracy: 0.73 - ETA: 1s - loss: 0.8442 - accuracy: 0.73 - ETA: 0s - loss: 0.8412 - accuracy: 0.73 - ETA: 0s - loss: 0.8403 - accuracy: 0.73 - ETA: 0s - loss: 0.8420 - accuracy: 0.72 - ETA: 0s - loss: 0.8342 - accuracy: 0.73 - ETA: 0s - loss: 0.8403 - accuracy: 0.72 - ETA: 0s - loss: 0.8360 - accuracy: 0.72 - ETA: 0s - loss: 0.8294 - accuracy: 0.73 - ETA: 0s - loss: 0.8271 - accuracy: 0.73 - ETA: 0s - loss: 0.8225 - accuracy: 0.73 - ETA: 0s - loss: 0.8185 - accuracy: 0.73 - ETA: 0s - loss: 0.8161 - accuracy: 0.73 - ETA: 0s - loss: 0.8183 - accuracy: 0.73 - ETA: 0s - loss: 0.8175 - accuracy: 0.73 - ETA: 0s - loss: 0.8169 - accuracy: 0.73 - 3s 34ms/step - loss: 0.8171 - accuracy: 0.7356 - val_loss: 0.7211 - val_accuracy: 0.7726\n",
      "Epoch 13/200\n",
      "73/73 [==============================] - ETA: 6s - loss: 0.6995 - accuracy: 0.75 - ETA: 2s - loss: 0.7632 - accuracy: 0.77 - ETA: 2s - loss: 0.8157 - accuracy: 0.76 - ETA: 1s - loss: 0.7838 - accuracy: 0.76 - ETA: 1s - loss: 0.7791 - accuracy: 0.76 - ETA: 1s - loss: 0.7912 - accuracy: 0.76 - ETA: 1s - loss: 0.7937 - accuracy: 0.75 - ETA: 1s - loss: 0.8083 - accuracy: 0.74 - ETA: 1s - loss: 0.8191 - accuracy: 0.74 - ETA: 1s - loss: 0.8152 - accuracy: 0.74 - ETA: 1s - loss: 0.8074 - accuracy: 0.74 - ETA: 0s - loss: 0.7986 - accuracy: 0.74 - ETA: 0s - loss: 0.7914 - accuracy: 0.74 - ETA: 0s - loss: 0.8034 - accuracy: 0.74 - ETA: 0s - loss: 0.8003 - accuracy: 0.74 - ETA: 0s - loss: 0.8073 - accuracy: 0.74 - ETA: 0s - loss: 0.8079 - accuracy: 0.74 - ETA: 0s - loss: 0.8063 - accuracy: 0.74 - ETA: 0s - loss: 0.8035 - accuracy: 0.74 - ETA: 0s - loss: 0.8063 - accuracy: 0.74 - ETA: 0s - loss: 0.8087 - accuracy: 0.74 - ETA: 0s - loss: 0.8078 - accuracy: 0.74 - ETA: 0s - loss: 0.8080 - accuracy: 0.74 - ETA: 0s - loss: 0.8141 - accuracy: 0.74 - ETA: 0s - loss: 0.8146 - accuracy: 0.74 - ETA: 0s - loss: 0.8174 - accuracy: 0.74 - 3s 34ms/step - loss: 0.8171 - accuracy: 0.7412 - val_loss: 0.7298 - val_accuracy: 0.7556\n",
      "Epoch 14/200\n",
      "73/73 [==============================] - ETA: 5s - loss: 0.6438 - accuracy: 0.84 - ETA: 2s - loss: 0.8249 - accuracy: 0.73 - ETA: 2s - loss: 0.8010 - accuracy: 0.75 - ETA: 2s - loss: 0.8479 - accuracy: 0.72 - ETA: 1s - loss: 0.8383 - accuracy: 0.72 - ETA: 1s - loss: 0.8161 - accuracy: 0.72 - ETA: 1s - loss: 0.8336 - accuracy: 0.72 - ETA: 1s - loss: 0.8202 - accuracy: 0.73 - ETA: 1s - loss: 0.8186 - accuracy: 0.72 - ETA: 1s - loss: 0.8158 - accuracy: 0.73 - ETA: 1s - loss: 0.8157 - accuracy: 0.72 - ETA: 1s - loss: 0.8129 - accuracy: 0.73 - ETA: 1s - loss: 0.8120 - accuracy: 0.73 - ETA: 0s - loss: 0.8050 - accuracy: 0.73 - ETA: 0s - loss: 0.8108 - accuracy: 0.73 - ETA: 0s - loss: 0.8145 - accuracy: 0.73 - ETA: 0s - loss: 0.8149 - accuracy: 0.73 - ETA: 0s - loss: 0.8170 - accuracy: 0.73 - ETA: 0s - loss: 0.8265 - accuracy: 0.73 - ETA: 0s - loss: 0.8232 - accuracy: 0.73 - ETA: 0s - loss: 0.8222 - accuracy: 0.73 - ETA: 0s - loss: 0.8181 - accuracy: 0.73 - ETA: 0s - loss: 0.8199 - accuracy: 0.73 - ETA: 0s - loss: 0.8155 - accuracy: 0.73 - ETA: 0s - loss: 0.8153 - accuracy: 0.73 - ETA: 0s - loss: 0.8146 - accuracy: 0.73 - ETA: 0s - loss: 0.8131 - accuracy: 0.73 - 3s 34ms/step - loss: 0.8129 - accuracy: 0.7384 - val_loss: 0.7595 - val_accuracy: 0.7701\n",
      "Epoch 15/200\n",
      "73/73 [==============================] - ETA: 6s - loss: 0.7741 - accuracy: 0.70 - ETA: 2s - loss: 0.8435 - accuracy: 0.72 - ETA: 2s - loss: 0.8692 - accuracy: 0.71 - ETA: 1s - loss: 0.8109 - accuracy: 0.74 - ETA: 1s - loss: 0.8015 - accuracy: 0.74 - ETA: 1s - loss: 0.8066 - accuracy: 0.74 - ETA: 1s - loss: 0.7974 - accuracy: 0.74 - ETA: 1s - loss: 0.7918 - accuracy: 0.74 - ETA: 1s - loss: 0.8025 - accuracy: 0.74 - ETA: 1s - loss: 0.7966 - accuracy: 0.74 - ETA: 1s - loss: 0.7997 - accuracy: 0.74 - ETA: 0s - loss: 0.7987 - accuracy: 0.74 - ETA: 0s - loss: 0.7978 - accuracy: 0.74 - ETA: 0s - loss: 0.7945 - accuracy: 0.74 - ETA: 0s - loss: 0.7967 - accuracy: 0.74 - ETA: 0s - loss: 0.7895 - accuracy: 0.75 - ETA: 0s - loss: 0.7857 - accuracy: 0.75 - ETA: 0s - loss: 0.7944 - accuracy: 0.75 - ETA: 0s - loss: 0.7931 - accuracy: 0.75 - ETA: 0s - loss: 0.7999 - accuracy: 0.74 - ETA: 0s - loss: 0.8040 - accuracy: 0.74 - ETA: 0s - loss: 0.8036 - accuracy: 0.74 - ETA: 0s - loss: 0.7982 - accuracy: 0.74 - ETA: 0s - loss: 0.8028 - accuracy: 0.74 - ETA: 0s - loss: 0.8067 - accuracy: 0.74 - ETA: 0s - loss: 0.8073 - accuracy: 0.74 - 3s 34ms/step - loss: 0.8076 - accuracy: 0.7425 - val_loss: 0.6961 - val_accuracy: 0.7880\n",
      "Epoch 16/200\n",
      "73/73 [==============================] - ETA: 5s - loss: 1.0180 - accuracy: 0.68 - ETA: 2s - loss: 0.8109 - accuracy: 0.75 - ETA: 2s - loss: 0.8152 - accuracy: 0.74 - ETA: 1s - loss: 0.8120 - accuracy: 0.71 - ETA: 1s - loss: 0.8122 - accuracy: 0.72 - ETA: 1s - loss: 0.8121 - accuracy: 0.71 - ETA: 1s - loss: 0.8165 - accuracy: 0.72 - ETA: 1s - loss: 0.8186 - accuracy: 0.72 - ETA: 1s - loss: 0.8188 - accuracy: 0.72 - ETA: 1s - loss: 0.8040 - accuracy: 0.73 - ETA: 1s - loss: 0.8112 - accuracy: 0.73 - ETA: 0s - loss: 0.8116 - accuracy: 0.73 - ETA: 0s - loss: 0.7961 - accuracy: 0.74 - ETA: 0s - loss: 0.8038 - accuracy: 0.74 - ETA: 0s - loss: 0.8006 - accuracy: 0.74 - ETA: 0s - loss: 0.8000 - accuracy: 0.74 - ETA: 0s - loss: 0.8036 - accuracy: 0.74 - ETA: 0s - loss: 0.8038 - accuracy: 0.74 - ETA: 0s - loss: 0.8062 - accuracy: 0.73 - ETA: 0s - loss: 0.7989 - accuracy: 0.74 - ETA: 0s - loss: 0.7921 - accuracy: 0.74 - ETA: 0s - loss: 0.7890 - accuracy: 0.74 - ETA: 0s - loss: 0.7827 - accuracy: 0.75 - ETA: 0s - loss: 0.7825 - accuracy: 0.75 - ETA: 0s - loss: 0.7847 - accuracy: 0.75 - ETA: 0s - loss: 0.7837 - accuracy: 0.75 - 2s 33ms/step - loss: 0.7843 - accuracy: 0.7528 - val_loss: 0.6866 - val_accuracy: 0.7949\n",
      "Epoch 17/200\n",
      "73/73 [==============================] - ETA: 5s - loss: 0.5686 - accuracy: 0.87 - ETA: 2s - loss: 0.7060 - accuracy: 0.79 - ETA: 2s - loss: 0.7628 - accuracy: 0.75 - ETA: 1s - loss: 0.8027 - accuracy: 0.75 - ETA: 1s - loss: 0.7810 - accuracy: 0.76 - ETA: 1s - loss: 0.7806 - accuracy: 0.76 - ETA: 1s - loss: 0.7732 - accuracy: 0.76 - ETA: 1s - loss: 0.7649 - accuracy: 0.76 - ETA: 1s - loss: 0.7751 - accuracy: 0.76 - ETA: 1s - loss: 0.7675 - accuracy: 0.77 - ETA: 1s - loss: 0.7677 - accuracy: 0.76 - ETA: 0s - loss: 0.7722 - accuracy: 0.76 - ETA: 0s - loss: 0.7714 - accuracy: 0.76 - ETA: 0s - loss: 0.7669 - accuracy: 0.76 - ETA: 0s - loss: 0.7657 - accuracy: 0.76 - ETA: 0s - loss: 0.7731 - accuracy: 0.76 - ETA: 0s - loss: 0.7788 - accuracy: 0.75 - ETA: 0s - loss: 0.7814 - accuracy: 0.75 - ETA: 0s - loss: 0.7784 - accuracy: 0.75 - ETA: 0s - loss: 0.7748 - accuracy: 0.75 - ETA: 0s - loss: 0.7727 - accuracy: 0.75 - ETA: 0s - loss: 0.7687 - accuracy: 0.75 - ETA: 0s - loss: 0.7700 - accuracy: 0.75 - ETA: 0s - loss: 0.7699 - accuracy: 0.75 - ETA: 0s - loss: 0.7683 - accuracy: 0.75 - ETA: 0s - loss: 0.7714 - accuracy: 0.75 - ETA: 0s - loss: 0.7756 - accuracy: 0.75 - 3s 35ms/step - loss: 0.7756 - accuracy: 0.7530 - val_loss: 0.6973 - val_accuracy: 0.7915\n",
      "Epoch 18/200\n",
      "53/73 [====================>.........] - ETA: 6s - loss: 0.7751 - accuracy: 0.76 - ETA: 2s - loss: 0.7537 - accuracy: 0.77 - ETA: 2s - loss: 0.8074 - accuracy: 0.75 - ETA: 2s - loss: 0.8009 - accuracy: 0.75 - ETA: 1s - loss: 0.7703 - accuracy: 0.75 - ETA: 1s - loss: 0.7611 - accuracy: 0.75 - ETA: 1s - loss: 0.7480 - accuracy: 0.76 - ETA: 1s - loss: 0.7594 - accuracy: 0.75 - ETA: 1s - loss: 0.7593 - accuracy: 0.76 - ETA: 1s - loss: 0.7532 - accuracy: 0.76 - ETA: 1s - loss: 0.7594 - accuracy: 0.76 - ETA: 0s - loss: 0.7619 - accuracy: 0.76 - ETA: 0s - loss: 0.7565 - accuracy: 0.76 - ETA: 0s - loss: 0.7632 - accuracy: 0.76 - ETA: 0s - loss: 0.7659 - accuracy: 0.76 - ETA: 0s - loss: 0.7665 - accuracy: 0.76 - ETA: 0s - loss: 0.7684 - accuracy: 0.76 - ETA: 0s - loss: 0.7657 - accuracy: 0.76 - ETA: 0s - loss: 0.7593 - accuracy: 0.76 - ETA: 0s - loss: 0.7567 - accuracy: 0.7639"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-723bf2352375>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m   \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m   \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1187\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \"\"\"\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unrecognized hook: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    313\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m       \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1026\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1028\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    510\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     \"\"\"\n\u001b[0;32m   1093\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1094\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1095\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1058\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1060\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1061\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=200\n",
    "history = model.fit(\n",
    "  train,\n",
    "  validation_data=test,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model as a SavedModel.\n",
    "# model.save('./data/humanModel/mode_v0')\n",
    "model.save('./data/'+map_dir+'/humanModel_v0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model in js format\n",
    "# import tensorflowjs as tfjs\n",
    "# tfjs.converters.save_keras_model(model, 'data/humanModel/js_model_v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = tf.keras.models.load_model('./data/humanModel/mode_v0')\n",
    "\n",
    "# # Check its architecture\n",
    "# new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/humanModel_dataset_split/val/38/-Mr4e3Fomo72jz0zfTgB_14.png\"\n",
    "\n",
    "img = keras.preprocessing.image.load_img(\n",
    "    path, target_size=(IMG_SIZE, IMG_SIZE)\n",
    ")\n",
    "img_array = keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)[0]\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(np.argmax(predictions), 100 * np.max(predictions))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad60775ae12af945b53c5c94c294ef05f229143b8585a0d23625a81e181fdccc"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('tf-gpu': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
