{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "map_dir = 'map 5'\n",
    "path = './data/'+map_dir+'/data-of-baselines.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyrebase\n",
    "\n",
    "# firebaseConfig = {\n",
    "#         \"apiKey\": \"AIzaSyAy6TmnVcLWjkpSpQFtCnX-PVGignQFsiw\",\n",
    "#     \"authDomain\": \"packman-game.firebaseapp.com\",\n",
    "#     \"databaseURL\": \"https://packman-game-default-rtdb.firebaseio.com\",\n",
    "#     \"projectId\": \"packman-game\",\n",
    "#     \"storageBucket\": \"packman-game.appspot.com\",\n",
    "#     \"messagingSenderId\": \"819894936980\",\n",
    "#     \"appId\": \"1:819894936980:web:7cbb8a8e4efb4e00d81b81\"\n",
    "#     };\n",
    "\n",
    "# firebase=pyrebase.initialize_app(firebaseConfig)\n",
    "\n",
    "# db=firebase.database()\n",
    "\n",
    "# # Get a database reference to our posts\n",
    "# ref = db.reference('/all-games')\n",
    "\n",
    "# # Read the data at the posts reference (this is a blocking operation)\n",
    "# print(ref.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from the json file\n",
    "with open(path) as train_file:\n",
    "    data = json.load(train_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survay results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "upload Id workers and search for duplicate workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_state_to_action = pd.DataFrame.from_dict(data['humanModel'], orient='index')\n",
    "raw_df_state_to_action = raw_df_state_to_action.drop(0, axis=1)\n",
    "\n",
    "participants_df = pd.DataFrame.from_dict(data['all-games'], orient='index')\n",
    "\n",
    "path = 'data/'+map_dir+'/workers' # use your path\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "li = []\n",
    "for filename in all_files:\n",
    "    fromOne = pd.read_csv(filename, index_col=None, header=0)\n",
    "    fromOne['filename'] = filename\n",
    "    li.append(fromOne)\n",
    "\n",
    "workers = pd.concat(li, axis=0, ignore_index=True)\n",
    "workers = workers.set_index('Answer.surveycode')\n",
    "workers.index = workers.index.map(lambda code: '-' + code[:-3])\n",
    "# print(workers.index[workers.index.duplicated()])\n",
    "workers = workers[~workers.index.duplicated(keep='first')]\n",
    "participants_df['WorkerId'] = workers['WorkerId']\n",
    "participants_df = participants_df.dropna(subset = ['WorkerId'])\n",
    "participants_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_drop = participants_df[participants_df.duplicated('WorkerId', keep='first')].index\n",
    "print(len(index_to_drop))\n",
    "\n",
    "participants_df = participants_df[~participants_df.duplicated('WorkerId', keep='first')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of people that answer the survay only once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(participants_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# participants_df[['computer_score', 'human_score', 'behavior']].to_excel(\"agents_score.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_df[['additional_comments', 'behavior']][participants_df['additional_comments'].notna()][participants_df['additional_comments'] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_df.loc[: ,'human_score'] = pd.to_numeric(participants_df['human_score'], errors='coerce')\n",
    "participants_df.loc[: ,'computer_score'] = pd.to_numeric(participants_df['computer_score'], errors='coerce')\n",
    "participants_df.loc[: ,'collaborative_value'] = pd.to_numeric(participants_df['collaborative_value'], errors='coerce')\n",
    "participants_df.loc[: ,'predictable_value'] = pd.to_numeric(participants_df['predictable_value'], errors='coerce')\n",
    "participants_df.loc[: ,'selfishly_value'] = pd.to_numeric(participants_df['selfishly_value'], errors='coerce')\n",
    "participants_df.loc[: ,'wisely_value'] = pd.to_numeric(participants_df['wisely_value'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_df.groupby('behavior')[['computer_score', 'human_score', 'collaborative_value', 'predictable_value', 'selfishly_value', 'wisely_value']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_df['behavior'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_df['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_df[participants_df['behavior'] == 'selfish'][['computer_score', 'human_score','education']].groupby('education').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove from the dataset :\n",
    "# raw_df_state_to_action = raw_df_state_to_action.drop(index_to_drop)\n",
    "raw_df_state_to_action.info(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAction(cell):\n",
    "    if cell != None:\n",
    "        return int(cell['action'])\n",
    "    return np.nan\n",
    "\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "def extractState(cell):\n",
    "    if cell == None:\n",
    "        return cell\n",
    "    board = np.array(cell['state'][0]).astype(float)\n",
    "    human_trace = np.array(cell['state'][1]).astype(float)\n",
    "    computer_trace = np.array(cell['state'][2]).astype(float)\n",
    "    human_awards = np.array(cell['state'][3]).astype(float)\n",
    "    computer_awards = np.array(cell['state'][4]).astype(float)\n",
    "    all_awards = np.array(cell['state'][5]).astype(float)\n",
    "    \n",
    "    r = human_awards/2 + human_trace + all_awards\n",
    "    g = board/3 + all_awards\n",
    "    b = computer_awards/2 + computer_trace + all_awards\n",
    "    rgb = np.dstack((r,g,b))\n",
    "    return NormalizeData(rgb)\n",
    "    \n",
    "\n",
    "state_df = pd.DataFrame(columns=raw_df_state_to_action.columns)\n",
    "action_df = pd.DataFrame(columns=raw_df_state_to_action.columns)\n",
    "for col in raw_df_state_to_action:\n",
    "    state_df[col] = raw_df_state_to_action[col].apply(extractState)\n",
    "    action_df[col] = raw_df_state_to_action[col].apply(extractAction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# view one game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowToImage(row):\n",
    "    fig = plt.figure(figsize=(40, 20))\n",
    "    row_s = state_df.loc[row, :]\n",
    "    row_a = action_df.loc[row, :]\n",
    "    cols_i = row_a.count()\n",
    "    for i in range(1, cols_i+1):\n",
    "        rows = 1\n",
    "        columns = cols_i\n",
    "        state = row_s.at[i]\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.imshow(state)\n",
    "        plt.axis('off')\n",
    "        action = row_a.at[i]\n",
    "        plt.title(str(i) + \" action: \" + str(action), fontsize=15)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_df[(action_df.count(axis=1) > 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_df[len(action_df.columns) + 1] = np.NaN\n",
    "action_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './data/humanModel_dataset'\n",
    "if os.path.exists(dir_path):\n",
    "    shutil.rmtree(dir_path)\n",
    "    os.mkdir(dir_path)\n",
    "    for action in ['32','37','38','39','40']:\n",
    "        os.mkdir(dir_path+'/'+action)\n",
    "else:\n",
    "    print(\"File not found in the directory\")\n",
    "\n",
    "counter = 0\n",
    "for (idxRow, s1), (_, s2) in zip(state_df.iterrows(), action_df.iterrows()):\n",
    "    for (idxCol, state), (_, action) in zip(s1.iteritems(), s2.iteritems()):\n",
    "        # check if it is not the last state\n",
    "        # the last state not enter our model, since it is an end state that not contains any dirts and not preform an action\n",
    "        if not np.isnan(action_df.loc[idxRow, idxCol+1]):\n",
    "            im = Image.fromarray((state * 255).astype(np.uint8))\n",
    "            path = f'data/humanModel_dataset/{int(action)}/{idxRow}_{idxCol}.png'\n",
    "            if counter % 500 == 0:\n",
    "                # print every 500 saved images\n",
    "                print(f'{idxRow}_{idxCol}.png saved! at action {action}')\n",
    "            counter += 1\n",
    "            im.save(path)\n",
    "        else:\n",
    "            break\n",
    "#         print (state, action, idxCol, idxRow)\n",
    "print(f'{counter} images have been saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rowToImage('-Mr4Yrtg2Rk8u83xa6yl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = \"-MqplpytsnX0zCv_KW9i\"\n",
    "# col = 3\n",
    "# plt.imshow(state_df.loc[index, col])\n",
    "# title = \"id: \" + index + \", col: \" + str(col) + \", action: \" + str(action_df.loc[index, col])\n",
    "# plt.title(title)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only for the old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop not finish game\n",
    "\n",
    "# --> TODO: not working!\n",
    "\n",
    "print(len( raw_df_state_to_action.index))\n",
    "# null_index = participants_df.loc[raw_df_state_to_action.index, :][participants_df.loc[raw_df_state_to_action.index, :]['computer_score'].isnull()].index.tolist()\n",
    "null_index = raw_df_state_to_action.index.difference(participants_df.index, sort=False)\n",
    "print(len(null_index))\n",
    "# print(null_index)\n",
    "raw_df_state_to_action_finish_game = raw_df_state_to_action.drop(null_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractState(cell):\n",
    "    if cell == None:\n",
    "        return cell\n",
    "    board = np.array(cell['state'][0]).astype(float)\n",
    "    human_trace = np.array(cell['state'][1]).astype(float)\n",
    "    computer_trace = np.array(cell['state'][2]).astype(float)\n",
    "    human_awards = np.array(cell['state'][3]).astype(float)\n",
    "    computer_awards = np.array(cell['state'][4]).astype(float)\n",
    "    all_awards = np.array(cell['state'][5]).astype(float)\n",
    "    return (board, human_trace, computer_trace, human_awards, computer_awards, all_awards)\n",
    "\n",
    "state_dim_6_df = pd.DataFrame(columns=raw_df_state_to_action_finish_game.columns)\n",
    "for col in raw_df_state_to_action_finish_game:\n",
    "    state_dim_6_df[col] = raw_df_state_to_action_finish_game[col].apply(extractState)\n",
    "\n",
    "state_dim_6_df[len(state_dim_6_df.columns) + 1] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countActions(row):\n",
    "    for i in range(1, len(row)+1):\n",
    "        if row[i] != None: # game not ended\n",
    "            continue\n",
    "        else:\n",
    "            # if there are i-1 states, then there are i-2\n",
    "            return i-2\n",
    "\n",
    "def countCollectedDirt(row, agent):\n",
    "    NotNullLastCol = row[1]\n",
    "    for i in range(2, len(row)+1):\n",
    "        if row[i] != None: # game not ended\n",
    "            NotNullLastCol = row[i]\n",
    "        else:\n",
    "            assert np.count_nonzero(NotNullLastCol[5]) == 0\n",
    "            if agent == 'computer':\n",
    "                return np.count_nonzero(NotNullLastCol[4])\n",
    "            else: # agent == 'human'\n",
    "                return np.count_nonzero(NotNullLastCol[3])\n",
    "\n",
    "def countStays(row, agent):\n",
    "    stay_count = 0\n",
    "    # if agent == 'computer':\n",
    "    #     last_pos = np.where(row[1][2] == 1)\n",
    "    # else:\n",
    "    #     last_pos = np.where(row[1][1] == 1)\n",
    "\n",
    "    for i in range(2, len(row)+1):\n",
    "        if row[i] != None: # game not ended\n",
    "            # if agent == 'computer':\n",
    "            #     current_pos = np.where(row[i][2] == 1)\n",
    "            # else: # agent == 'human'\n",
    "            #     current_pos = np.where(row[i][1] == 1)\n",
    "            # if current_pos == last_pos:\n",
    "            #     stay_count += 1\n",
    "            # else:\n",
    "            #     last_pos = current_pos\n",
    "            \n",
    "            # faster way\n",
    "            if agent == 'computer':\n",
    "                trace_agent = row[i][2]\n",
    "            else: # agent == 'human'\n",
    "                trace_agent = row[i][1]\n",
    "            if 0.9 not in trace_agent:\n",
    "                stay_count += 1\n",
    "        else:\n",
    "            return stay_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_scores(df):\n",
    "    rewards = {\n",
    "        'Start': 0.5,\n",
    "        'Stay': -0.01,\n",
    "        'Move': -0.03,\n",
    "        'EndGame': 1.0\n",
    "    }\n",
    "    c_h_scores = pd.DataFrame(index=df.index, columns=['computer_score', 'human_score'])\n",
    "    computer_scores = []\n",
    "    human_scores = []\n",
    "    for i, row in df.iterrows():\n",
    "        num_of_actions = countActions(row)\n",
    "        computer_num_of_dirt = countCollectedDirt(row, 'computer')\n",
    "        human_num_of_dirt = countCollectedDirt(row, 'human')\n",
    "        computer_num_of_stays = countStays(row, 'computer')\n",
    "        human_num_of_stays = countStays(row, 'human')\n",
    "\n",
    "        # if i == '-Mr4RiLyH06mIlLkOkTw':\n",
    "        #     print('num_of_actions: ', num_of_actions)\n",
    "        #     print('computer_num_of_dirt: ', computer_num_of_dirt)\n",
    "        #     print('human_num_of_dirt: ',human_num_of_dirt)\n",
    "        #     print('computer_num_of_stays: ', computer_num_of_stays)\n",
    "        #     print('human_num_of_stays: ',human_num_of_stays)\n",
    "        #     # raise RuntimeError\n",
    "\n",
    "        coputerScore = rewards['Start'] + (num_of_actions - computer_num_of_stays - computer_num_of_dirt) * rewards['Move'] + computer_num_of_stays * rewards['Stay']\n",
    "        coputerScore += rewards['EndGame']\n",
    "        humanScore = rewards['Start'] + (num_of_actions - human_num_of_stays - human_num_of_dirt) * rewards['Move'] + human_num_of_stays * rewards['Stay']\n",
    "        humanScore += rewards['EndGame']\n",
    "        computer_scores.append(coputerScore)\n",
    "        human_scores.append(humanScore)\n",
    "\n",
    "    c_h_scores['computer_score'] = computer_scores\n",
    "    c_h_scores['human_score'] = human_scores\n",
    "    c_h_scores['computer_score_real'] = participants_df['computer_score']\n",
    "    c_h_scores['human_score_real'] = participants_df['human_score']\n",
    "    c_h_scores['behavior'] = participants_df['behavior']\n",
    "    return c_h_scores\n",
    "\n",
    "new_scores_df = fix_scores(state_dim_6_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scores_df.groupby('behavior').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_collected_dirt_score(df):\n",
    "    rewards = {\n",
    "        'Collected_dirt': 0.05,\n",
    "    }\n",
    "    c_h_scores = pd.DataFrame(index=df.index, columns=['computer_score', 'human_score'])\n",
    "    computer_scores = []\n",
    "    human_scores = []\n",
    "    for i, row in df.iterrows():\n",
    "        computer_num_of_dirt = countCollectedDirt(row, 'computer')\n",
    "        human_num_of_dirt = countCollectedDirt(row, 'human')\n",
    "\n",
    "        coputerScore = computer_num_of_dirt * rewards['Collected_dirt']\n",
    "        humanScore = human_num_of_dirt * rewards['Collected_dirt']\n",
    "        computer_scores.append(participants_df.loc[i, 'computer_score'] - coputerScore)\n",
    "        human_scores.append(participants_df.loc[i, 'human_score'] - humanScore)\n",
    "\n",
    "    c_h_scores['computer_score'] = computer_scores\n",
    "    c_h_scores['human_score'] = human_scores\n",
    "    c_h_scores['computer_score_real'] = participants_df['computer_score']\n",
    "    c_h_scores['human_score_real'] = participants_df['human_score']\n",
    "    c_h_scores['behavior'] = participants_df['behavior']\n",
    "    return c_h_scores\n",
    "\n",
    "new_scores_df = fix_collected_dirt_score(state_dim_6_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scores_df.groupby('behavior').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# participants_df['computer_score'] = new_scores_df['computer_score']\n",
    "# participants_df['human_score'] = new_scores_df['human_score']\n",
    "# participants_df.to_json(r'participants_df.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scores_df.to_excel(\"data/\"+map_dir+\"/all_agents_score.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e48d23369a553edc96f1373b6255b5687d82d68cd08867622b2500e338930542"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
