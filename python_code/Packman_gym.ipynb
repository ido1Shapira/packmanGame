{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build Packman Environment with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class PackmanEnv(Env):\n",
    "    rewards = {\n",
    "        'Start': 50,\n",
    "        0: -1,\n",
    "        1: -2,\n",
    "        2: -2,\n",
    "        3: -2,\n",
    "        4: -2,\n",
    "        'CollectDirt': 2,  # (-2 + 2 = 0)\n",
    "        'EndGame': 100\n",
    "    }\n",
    "    actions = {\n",
    "        0: \"Stay\",\n",
    "        1: \"Left\",\n",
    "        2: \"Up\",\n",
    "        3: \"Right\",\n",
    "        4: \"Down\"\n",
    "    }\n",
    "    def __init__(self):\n",
    "        # Actions we can take, left, down, stay, up, right\n",
    "        self.action_space = Discrete(5)\n",
    "        \n",
    "        # Define a 2-D observation space\n",
    "        self.observation_shape = (10, 10, 3)\n",
    "        self.observation_space = Box(low=np.zeros(self.observation_shape),\n",
    "                                            high=np.ones(self.observation_shape),\n",
    "                                            dtype=np.float32)\n",
    "        # Set start state\n",
    "        self.dict_state = None\n",
    "        self.canvas = None\n",
    "        \n",
    "        # Load human model from the computer\n",
    "        self.human_model = tf.keras.models.load_model('./data/humanModel/mode_v0')\n",
    "    \n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0: \"Stay\"\n",
    "        # 1: \"Left\"\n",
    "        # 2: \"Up\"\n",
    "        # 3: \"Right\"\n",
    "        # 4: \"Down\"\n",
    "        computer_reward = 0\n",
    "        human_reward = 0\n",
    "        \n",
    "        # Assert that it is a valid action \n",
    "        assert self.action_space.contains(action), \"Invalid Action\"\n",
    "        \n",
    "        human_pos = np.where(self.dict_state['Human trace'] == 1)\n",
    "        computer_pos = np.where(self.dict_state['Computer trace'] == 1)\n",
    "        # predict next human action\n",
    "        \n",
    "        # when human model is ready uncomment this line\n",
    "#         human_action = self.predict_action(self.canvas)\n",
    "        human_action = self.get_random_valid_action(human_pos)\n",
    "    \n",
    "        assert self.valid_action(action, computer_pos) , \"Computer preformed invalid action: \" + str(action) + \" at pos: \" + str(computer_pos)\n",
    "        assert self.valid_action(human_action, human_pos) , \"Human preformed invalid action: \" + str(human_action) + \" at pos: \" + str(computer_pos)\n",
    "    \n",
    "        self.move(human_action, 'human') # assume human action is valid\n",
    "        # apply the action to the agent\n",
    "        self.move(action, 'computer')\n",
    "        \n",
    "        # check for clean dirt for both agents\n",
    "        dirts_pos = np.where(self.dict_state['All awards'] == 1)\n",
    "        for dirt_pos_i, dirt_pos_j in zip(dirts_pos[0], dirts_pos[1]):\n",
    "            if human_pos[0][0] == dirt_pos_i and human_pos[1][0] == dirt_pos_j:\n",
    "                self.dict_state['All awards'][human_pos] = 0\n",
    "                self.dict_state['Human awards'][human_pos] = 1\n",
    "                human_reward += self.rewards['CollectDirt']\n",
    "            if computer_pos[0][0] == dirt_pos_i and computer_pos[1][0] == dirt_pos_j:\n",
    "                self.dict_state['All awards'][computer_pos] = 0\n",
    "                self.dict_state['Computer awards'][computer_pos] = 1\n",
    "                computer_reward += self.rewards['CollectDirt']\n",
    "                            \n",
    "        # Reward for executing an action.\n",
    "        computer_reward = self.rewards[action]\n",
    "        human_reward = self.rewards[human_action]\n",
    "\n",
    "        if not np.any(self.dict_state['All awards']): # game ended when there is no dirt to clean\n",
    "            computer_reward += self.rewards['EndGame']\n",
    "            human_reward += self.rewards['EndGame']\n",
    "            done = True\n",
    "        else:\n",
    "            # Flag that marks the termination of an episode\n",
    "            done = False\n",
    "        \n",
    "        self.ep_return += computer_reward\n",
    "        self.ep_human_reward += human_reward\n",
    "        \n",
    "        self.canvas = self.convertToImage(self.dict_state)\n",
    "        \n",
    "        # Set placeholder for info\n",
    "        info = {\n",
    "                'done': done,\n",
    "               'current_reward': computer_reward,\n",
    "               'ep_return': self.ep_return,\n",
    "               'human_return':self.ep_human_reward,\n",
    "               }\n",
    "        \n",
    "        # Return step information\n",
    "        return self.canvas, computer_reward, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Implement viz\n",
    "        self.canvas = self.convertToImage(self.dict_state)\n",
    "        # Render the environment to the screen\n",
    "        if mode == 'human':\n",
    "            if self.call_once:\n",
    "                self.call_once = False\n",
    "                plt.figure(figsize=(8,8))\n",
    "                self.img = plt.imshow(self.canvas) # only call this once\n",
    "\n",
    "            self.img.set_data(self.canvas) # just update the data\n",
    "            plt.axis('off')\n",
    "            info = {\n",
    "               'ep_return': self.ep_return,\n",
    "               'human_return':self.ep_human_reward,\n",
    "            }\n",
    "            plt.title(f'info: {info}')\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        elif mode == 'rgb_array':\n",
    "            return self.canvas\n",
    "            \n",
    "    def reset(self):\n",
    "        self.call_once = True\n",
    "        # Reset board game\n",
    "        self.dict_state = self.init_state()\n",
    "        self.canvas = self.convertToImage(self.dict_state)\n",
    "        # Reset the reward\n",
    "        self.ep_return = self.rewards['Start']\n",
    "        self.ep_human_reward = self.rewards['Start']\n",
    "        \n",
    "        return self.canvas, self.ep_return\n",
    "    \n",
    "    \n",
    "    #################### Helper functions ########################\n",
    "    \n",
    "    def init_state(self):\n",
    "        # init board state with random n=5 dirt position\n",
    "        board = np.array([\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                    [0, 1, 1, 1, 1, 0, 1, 1, 1, 0],\n",
    "                    [0, 0, 1, 1, 0, 0, 0, 0, 1, 0],\n",
    "                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "                    [0, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
    "                    [0, 1, 0, 1, 0, 1, 0, 0, 1, 0],\n",
    "                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "                    [0, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n",
    "                    [0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "                ])\n",
    "        human_trace = np.zeros(board.shape)\n",
    "        human_trace[2][2] = 1 # locate human player\n",
    "        board[2][2] = 0 # locate human player\n",
    "        \n",
    "        computer_trace = np.zeros(board.shape)\n",
    "        computer_trace[7][7] = 1 # locate computer player\n",
    "        board[7][7] = 0 # locate computer player\n",
    "        \n",
    "        human_awards = np.zeros(board.shape)\n",
    "        computer_awards = np.zeros(board.shape)\n",
    "\n",
    "        all_awards = np.zeros(board.shape)\n",
    "        idx = np.random.choice(np.count_nonzero(board), 5)\n",
    "        all_awards[tuple(map(lambda x: x[idx], np.where(board)))] = 1\n",
    "        board[2][2] = 1 # locate human player\n",
    "        board[7][7] = 1 # locate computer player\n",
    "        \n",
    "        return {\n",
    "                'Board': board,\n",
    "                'Human trace': human_trace,\n",
    "                'Computer trace': computer_trace,\n",
    "                'Human awards': human_awards,\n",
    "                'Computer awards': computer_awards,\n",
    "                'All awards': all_awards,\n",
    "                }\n",
    "    \n",
    "    def get_random_valid_action(self, pos):\n",
    "        random_action = self.action_space.sample()\n",
    "        while not self.valid_action(random_action, pos):\n",
    "            random_action = self.action_space.sample()\n",
    "        return random_action\n",
    "    \n",
    "    def valid_action(self, action, pos):\n",
    "        next_pos = self.new_pos(pos, action)\n",
    "        if self.dict_state['Board'][next_pos] == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def new_pos(self, current_pos, action):\n",
    "            if action == 0: # stay\n",
    "                return current_pos\n",
    "            elif action == 1: # left\n",
    "                return (current_pos[0], current_pos[1]-1)\n",
    "            elif action == 2: # up\n",
    "                return (current_pos[0]-1, current_pos[1])\n",
    "            elif action == 3: # right\n",
    "                return (current_pos[0], current_pos[1]+1)\n",
    "            elif action == 4: # down\n",
    "                return (current_pos[0]+1, current_pos[1])\n",
    "            else: # default case if action is not found\n",
    "                assert True , \"action: \" + str(action) + \" is not vallid at agent pos: \" + str(current_pos) + \" for agent: \" + str(agent) \n",
    "                \n",
    "    def move(self, action, agent):\n",
    "        # assume action is valid\n",
    "        if agent == 'human':\n",
    "            current_pos = np.where(self.dict_state['Human trace'] == 1)\n",
    "            self.dict_state['Human trace'] = self.dict_state['Human trace']*0.9\n",
    "            next_pos = self.new_pos(current_pos, action)\n",
    "            self.dict_state['Human trace'][next_pos] = 1\n",
    "        elif agent == 'computer':\n",
    "            current_pos = np.where(self.dict_state['Computer trace'] == 1)\n",
    "            self.dict_state['Computer trace'] = self.dict_state['Computer trace']*0.9\n",
    "            next_pos = self.new_pos(current_pos, action) \n",
    "            self.dict_state['Computer trace'][next_pos] = 1\n",
    "        else:\n",
    "            assert True , \"agent not define:\" + str(agent)\n",
    "    \n",
    "    def predict_action(self, img):\n",
    "        img_array = keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "        predictions = self.human_model.predict(img_array)\n",
    "        score = tf.nn.softmax(predictions[0])\n",
    "        \n",
    "        action = np.argmax(score)\n",
    "#         print(\n",
    "#             \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "#             .format(action, 100 * np.max(score))\n",
    "#         )\n",
    "        return action\n",
    "\n",
    "    def convertToImage(self, state):\n",
    "        r = state['Human awards']/2 + state['Human trace']\n",
    "        g = state['Board']/3 + state['All awards']\n",
    "        b = state['Computer awards']/2 + state['Computer trace']\n",
    "        rgb = np.dstack((r,g,b))\n",
    "        return (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb)) # NormalizeImage\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "env = PackmanEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a69ae78bf964>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicies\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMlpPolicy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDQN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mACER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macktr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mACKTR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepq\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDQN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mher\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mppo2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPPO2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\deepq\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicies\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMlpPolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCnnPolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLnMlpPolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLnCnnPolicy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_graph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuild_act\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_train\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdqn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDQN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPrioritizedReplayBuffer\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\deepq\\policies.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspaces\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDiscrete\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines import DQN\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = DQN(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=25000)\n",
    "model.save(\"deepq_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = DQN.load(\"deepq_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\env_checker.py:25: UserWarning: It seems that your observation is an image but the `dtype` of your observation_space is not `np.uint8`. If your observation is not an image, we recommend you to flatten the observation to have only a 1D vector\n",
      "  warnings.warn(\"It seems that your observation is an image but the `dtype` \"\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\env_checker.py:31: UserWarning: It seems that your observation space is an image but the upper and lower bounds are not in [0, 255]. Because the CNN policy normalize automatically the observation you may encounter issue if the values are not in that range.\n",
      "  warnings.warn(\"It seems that your observation space is an image but the \"\n",
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\env_checker.py:38: UserWarning: The minimal resolution for an image is 36x36 for the default CnnPolicy. You might need to use a custom `cnn_extractor` cf https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html\n",
      "  warnings.warn(\"The minimal resolution for an image is 36x36 for the default CnnPolicy. \"\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "The observation returned by the `reset()` method should be a single value, not a tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3b180f999adc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# It will check your custom environment and output additional warnings if needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\env_checker.py\u001b[0m in \u001b[0;36mcheck_env\u001b[1;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;31m# ============ Check the returned values ===============\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[0m_check_returned_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;31m# ==== Check the render method and the declared render modes ====\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\env_checker.py\u001b[0m in \u001b[0;36m_check_returned_values\u001b[1;34m(env, observation_space, action_space)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0m_check_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;31m# Sample a random action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\stable_baselines\\common\\env_checker.py\u001b[0m in \u001b[0;36m_check_obs\u001b[1;34m(obs, observation_space, method_name)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         assert not isinstance(obs, tuple), (\"The observation returned by the `{}()` \"\n\u001b[1;32m---> 79\u001b[1;33m                                             \"method should be a single value, not a tuple\".format(method_name))\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# The check for a GoalEnv is done by the base class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: The observation returned by the `reset()` method should be a single value, not a tuple"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.env_checker import check_env\n",
    "\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "print(check_env(env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:2 =============> Score:-290\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "episodes = 2\n",
    "for episode in range(1, episodes+1):\n",
    "    state, score = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        computer_pos = np.where(env.dict_state['Computer trace'] == 1)\n",
    "        action = env.get_random_valid_action(computer_pos)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        env.render(mode='human')\n",
    "        \n",
    "        score += reward\n",
    "#         print(info)\n",
    "    print('Episode:{} =============> Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_states, num_actions):\n",
    "    model = Sequential()\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    model.add(Input(shape=(num_states)))\n",
    "              \n",
    "    model.add(Conv2D(filters=6, kernel_size=(3,3),input_shape=num_states, activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "              \n",
    "    model.add(Conv2D(filters=4, kernel_size=(3,3),input_shape=num_states, activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_actions, activation=\"softmax\", kernel_initializer=last_init))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 6)         168       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 6)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 5, 4)           220       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 4)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 473\n",
      "Trainable params: 473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym \n",
    "# env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rl.agents import DQNAgent\n",
    "# from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "# from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_agent(model, actions):\n",
    "#     policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=10000)\n",
    "#     memory = SequentialMemory(limit=50000, window_length=1)\n",
    "#     dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "#                   nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "#     return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (1, 1, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-08c9e140f6bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdqn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rl\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;31m# This is were all of the work happens. We first perceive and compute the action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;31m# (forward step) and then use the reward to improve (backward step).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;31m# Select an action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;31m# Validate and standardize user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m     inputs, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1188\u001b[1;33m         x, extract_tensors_from_dataset=True)\n\u001b[0m\u001b[0;32m   1189\u001b[0m     \u001b[1;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m     \u001b[1;31m# at this point.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2333\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2335\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2337\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2360\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2361\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2362\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2364\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    634\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    637\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (1, 1, 2)"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (1, 1, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-2331bfdbc6f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'episode_reward'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rl\\core.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, env, nb_episodes, action_repetition, callbacks, visualize, nb_max_episode_steps, nb_max_start_steps, start_step_policy, verbose)\u001b[0m\n\u001b[0;32m    339\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m                     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;31m# Select an action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;31m# Validate and standardize user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m     inputs, _, _ = self._standardize_user_data(\n\u001b[1;32m-> 1188\u001b[1;33m         x, extract_tensors_from_dataset=True)\n\u001b[0m\u001b[0;32m   1189\u001b[0m     \u001b[1;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m     \u001b[1;31m# at this point.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2333\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2335\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2337\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2360\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2361\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2362\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2364\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    634\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    637\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (1, 1, 2)"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reloading Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
