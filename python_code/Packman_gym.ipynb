{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build Packman Environment with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class PackmanEnv(Env):\n",
    "    rewards = {\n",
    "        'Start': 50,\n",
    "        0: -1,\n",
    "        1: -2,\n",
    "        2: -2,\n",
    "        3: -2,\n",
    "        4: -2,\n",
    "        'CollectDirt': 2,  # (-2 + 2 = 0)\n",
    "        'EndGame': 100\n",
    "    }\n",
    "    actions = {\n",
    "        0: \"Stay\",\n",
    "        1: \"Left\",\n",
    "        2: \"Up\",\n",
    "        3: \"Right\",\n",
    "        4: \"Down\"\n",
    "    }\n",
    "    def __init__(self):\n",
    "        # Actions we can take, left, down, stay, up, right\n",
    "        self.action_space = Discrete(5)\n",
    "        \n",
    "        # Define a 2-D observation space\n",
    "        self.observation_shape = (10, 10, 3)\n",
    "        self.observation_space = Box(low=np.zeros(self.observation_shape),\n",
    "                                            high=np.ones(self.observation_shape),\n",
    "                                            dtype=np.float32)\n",
    "        # Set start state\n",
    "        self.dict_state = None\n",
    "        self.canvas = None\n",
    "        \n",
    "        # Load human model from the computer\n",
    "        self.human_model = tf.keras.models.load_model('./data/humanModel/mode_v0')\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0: \"Stay\"\n",
    "        # 1: \"Left\"\n",
    "        # 2: \"Up\"\n",
    "        # 3: \"Right\"\n",
    "        # 4: \"Down\"\n",
    "        computer_reward = 0\n",
    "        human_reward = 0\n",
    "        \n",
    "        # Assert that it is a valid action \n",
    "        assert self.action_space.contains(action), \"Invalid Action\"\n",
    "        \n",
    "        human_pos = np.where(self.dict_state['Human trace'] == 1)\n",
    "        computer_pos = np.where(self.dict_state['Computer trace'] == 1)\n",
    "        # predict next human action\n",
    "        \n",
    "        # when human model is ready uncomment this line\n",
    "#         human_action = self.predict_action(self.canvas)\n",
    "        human_action = self.get_random_valid_action(human_pos)\n",
    "    \n",
    "        assert self.valid_action(action, computer_pos) , \"Computer preformed invalid action: \" + str(action) + \" at pos: \" + str(computer_pos)\n",
    "        assert self.valid_action(human_action, human_pos) , \"Human preformed invalid action: \" + str(human_action) + \" at pos: \" + str(computer_pos)\n",
    "    \n",
    "        self.move(human_action, 'human') # assume human action is valid\n",
    "        # apply the action to the agent\n",
    "        self.move(action, 'computer')\n",
    "        \n",
    "        # check for clean dirt for both agents\n",
    "        dirts_pos = np.where(self.dict_state['All awards'] == 1)\n",
    "        for dirt_pos_i, dirt_pos_j in zip(dirts_pos[0], dirts_pos[1]):\n",
    "            if human_pos[0][0] == dirt_pos_i and human_pos[1][0] == dirt_pos_j:\n",
    "                self.dict_state['All awards'][human_pos] = 0\n",
    "                self.dict_state['Human awards'][human_pos] = 1\n",
    "                human_reward += self.rewards['CollectDirt']\n",
    "            if computer_pos[0][0] == dirt_pos_i and computer_pos[1][0] == dirt_pos_j:\n",
    "                self.dict_state['All awards'][computer_pos] = 0\n",
    "                self.dict_state['Computer awards'][computer_pos] = 1\n",
    "                computer_reward += self.rewards['CollectDirt']\n",
    "                            \n",
    "        # Reward for executing an action.\n",
    "        computer_reward = self.rewards[action]\n",
    "        human_reward = self.rewards[human_action]\n",
    "\n",
    "        if not np.any(self.dict_state['All awards']): # game ended when there is no dirt to clean\n",
    "            computer_reward += self.rewards['EndGame']\n",
    "            human_reward += self.rewards['EndGame']\n",
    "            done = True\n",
    "        else:\n",
    "            # Flag that marks the termination of an episode\n",
    "            done = False\n",
    "        \n",
    "        self.ep_return += computer_reward\n",
    "        self.ep_human_reward += human_reward\n",
    "        \n",
    "        self.canvas = self.convertToImage(self.dict_state)\n",
    "        \n",
    "        # Set placeholder for info\n",
    "        info = {\n",
    "                'done': done,\n",
    "               'current_reward': computer_reward,\n",
    "               'ep_return': self.ep_return,\n",
    "               'human_return':self.ep_human_reward,\n",
    "               }\n",
    "        \n",
    "        # Return step information\n",
    "        return self.canvas, computer_reward, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Implement viz\n",
    "        self.canvas = self.convertToImage(self.dict_state)\n",
    "        # Render the environment to the screen\n",
    "        if mode == 'human':\n",
    "            if self.call_once:\n",
    "                self.call_once = False\n",
    "                plt.figure(figsize=(8,8))\n",
    "                self.img = plt.imshow(self.canvas) # only call this once\n",
    "\n",
    "            self.img.set_data(self.canvas) # just update the data\n",
    "            plt.axis('off')\n",
    "            info = {\n",
    "               'ep_return': self.ep_return,\n",
    "               'human_return':self.ep_human_reward,\n",
    "            }\n",
    "            plt.title(f'info: {info}')\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        elif mode == 'rgb_array':\n",
    "            return self.canvas\n",
    "            \n",
    "    def reset(self):\n",
    "        self.call_once = True\n",
    "        # Reset board game\n",
    "        self.dict_state = self.init_state()\n",
    "        self.canvas = self.convertToImage(self.dict_state)\n",
    "        # Reset the reward\n",
    "        self.ep_return = self.rewards['Start']\n",
    "        self.ep_human_reward = self.rewards['Start']\n",
    "        \n",
    "        return self.canvas, self.ep_return\n",
    "    \n",
    "    \n",
    "    #################### Helper functions ########################\n",
    "    \n",
    "    def init_state(self):\n",
    "        # init board state with random n=5 dirt position\n",
    "        board = np.array([\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                    [0, 1, 1, 1, 1, 0, 1, 1, 1, 0],\n",
    "                    [0, 0, 1, 1, 0, 0, 0, 0, 1, 0],\n",
    "                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "                    [0, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
    "                    [0, 1, 0, 1, 0, 1, 0, 0, 1, 0],\n",
    "                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "                    [0, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n",
    "                    [0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "                ])\n",
    "        human_trace = np.zeros(board.shape)\n",
    "        human_trace[2][2] = 1 # locate human player\n",
    "        board[2][2] = 0 # locate human player\n",
    "        \n",
    "        computer_trace = np.zeros(board.shape)\n",
    "        computer_trace[7][7] = 1 # locate computer player\n",
    "        board[7][7] = 0 # locate computer player\n",
    "        \n",
    "        human_awards = np.zeros(board.shape)\n",
    "        computer_awards = np.zeros(board.shape)\n",
    "\n",
    "        all_awards = np.zeros(board.shape)\n",
    "        idx = np.random.choice(np.count_nonzero(board), 5)\n",
    "        all_awards[tuple(map(lambda x: x[idx], np.where(board)))] = 1\n",
    "        board[2][2] = 1 # locate human player\n",
    "        board[7][7] = 1 # locate computer player\n",
    "        \n",
    "        return {\n",
    "                'Board': board,\n",
    "                'Human trace': human_trace,\n",
    "                'Computer trace': computer_trace,\n",
    "                'Human awards': human_awards,\n",
    "                'Computer awards': computer_awards,\n",
    "                'All awards': all_awards,\n",
    "                }\n",
    "    \n",
    "    def get_random_valid_action(self, pos):\n",
    "        random_action = self.action_space.sample()\n",
    "        while not self.valid_action(random_action, pos):\n",
    "            random_action = self.action_space.sample()\n",
    "        return random_action\n",
    "    \n",
    "    def valid_action(self, action, pos):\n",
    "        next_pos = self.new_pos(pos, action)\n",
    "        if self.dict_state['Board'][next_pos] == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def new_pos(self, current_pos, action):\n",
    "            if action == 0: # stay\n",
    "                return current_pos\n",
    "            elif action == 1: # left\n",
    "                return (current_pos[0], current_pos[1]-1)\n",
    "            elif action == 2: # up\n",
    "                return (current_pos[0]-1, current_pos[1])\n",
    "            elif action == 3: # right\n",
    "                return (current_pos[0], current_pos[1]+1)\n",
    "            elif action == 4: # down\n",
    "                return (current_pos[0]+1, current_pos[1])\n",
    "            else: # default case if action is not found\n",
    "                assert True , \"action: \" + str(action) + \" is not vallid at agent pos: \" + str(current_pos) + \" for agent: \" + str(agent) \n",
    "                \n",
    "    def move(self, action, agent):\n",
    "        # assume action is valid\n",
    "        if agent == 'human':\n",
    "            current_pos = np.where(self.dict_state['Human trace'] == 1)\n",
    "            self.dict_state['Human trace'] = self.dict_state['Human trace']*0.9\n",
    "            next_pos = self.new_pos(current_pos, action)\n",
    "            self.dict_state['Human trace'][next_pos] = 1\n",
    "        elif agent == 'computer':\n",
    "            current_pos = np.where(self.dict_state['Computer trace'] == 1)\n",
    "            self.dict_state['Computer trace'] = self.dict_state['Computer trace']*0.9\n",
    "            next_pos = self.new_pos(current_pos, action) \n",
    "            self.dict_state['Computer trace'][next_pos] = 1\n",
    "        else:\n",
    "            assert True , \"agent not define:\" + str(agent)\n",
    "    \n",
    "    def predict_action(self, img):\n",
    "        img_array = keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "        predictions = self.human_model.predict(img_array)\n",
    "        score = tf.nn.softmax(predictions[0])\n",
    "        \n",
    "        action = np.argmax(score)\n",
    "#         print(\n",
    "#             \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "#             .format(action, 100 * np.max(score))\n",
    "#         )\n",
    "        return action\n",
    "\n",
    "    def convertToImage(self, state):\n",
    "        r = state['Human awards']/2 + state['Human trace']\n",
    "        g = state['Board']/3 + state['All awards']\n",
    "        b = state['Computer awards']/2 + state['Computer trace']\n",
    "        rgb = np.dstack((r,g,b))\n",
    "        return (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb)) # NormalizeImage\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "source": [
    "env = PackmanEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 =============> Score:-10\n",
      "Episode:2 =============> Score:-295\n",
      "Episode:3 =============> Score:-552\n",
      "Episode:4 =============> Score:-574\n",
      "Episode:5 =============> Score:-616\n",
      "Episode:6 =============> Score:13\n",
      "Episode:7 =============> Score:-612\n",
      "Episode:8 =============> Score:-339\n",
      "Episode:9 =============> Score:45\n",
      "Episode:10 =============> Score:-260\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state, score = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        computer_pos = np.where(env.dict_state['Computer trace'] == 1)\n",
    "        action = env.get_random_valid_action(computer_pos)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        env.render(mode='rgb_array')\n",
    "        \n",
    "        score += reward\n",
    "#         print(info)\n",
    "    print('Episode:{} =============> Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(states, actions):\n",
    "#     model = tensorflow.keras.models.Sequential()\n",
    "#     model.add(Flatten(input_shape=states))\n",
    "#     model.add(Dense(24, activation='relu'))\n",
    "#     model.add(Dense(24, activation='relu'))\n",
    "#     model.add(Dense(actions, activation='linear'))\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_states, num_actions):\n",
    "    model = Sequential()\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "    model.add(Input(shape=(num_states)))\n",
    "              \n",
    "    model.add(Conv2D(filters=6, kernel_size=(3,3),input_shape=num_states, activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "              \n",
    "    model.add(Conv2D(filters=4, kernel_size=(3,3),input_shape=num_states, activation='relu', padding=\"same\"))\n",
    "    model.add(MaxPool2D(pool_size=2, strides=2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_actions, activation=\"softmax\", kernel_initializer=last_init))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 10, 10, 6)         168       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 6)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 5, 5, 4)           220       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 2, 2, 4)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 473\n",
      "Trainable params: 473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym \n",
    "# env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rl.agents import DQNAgent\n",
    "# from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "# from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_agent(model, actions):\n",
    "#     policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=10000)\n",
    "#     memory = SequentialMemory(limit=50000, window_length=1)\n",
    "#     dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "#                   nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "#     return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_2 to have 4 dimensions, but got array with shape (1, 1, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-08c9e140f6bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdqn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\rl\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;31m# This is were all of the work happens. We first perceive and compute the action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;31m# (forward step) and then use the reward to improve (backward step).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;31m# Select an action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1198\u001b[0m           ' tf.distribute.Strategy.')\n\u001b[0;32m   1199\u001b[0m     \u001b[1;31m# Validate and standardize user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1200\u001b[1;33m     inputs, _, _ = self._standardize_user_data(\n\u001b[0m\u001b[0;32m   1201\u001b[0m         x, extract_tensors_from_dataset=True)\n\u001b[0;32m   1202\u001b[0m     \u001b[1;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2326\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2328\u001b[1;33m     return self._standardize_tensors(\n\u001b[0m\u001b[0;32m   2329\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2330\u001b[0m         \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2355\u001b[0m       \u001b[1;31m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m       x = training_utils.standardize_input_data(\n\u001b[0m\u001b[0;32m   2357\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m           \u001b[0mfeed_input_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m           raise ValueError('Error when checking ' + exception_prefix +\n\u001b[0m\u001b[0;32m    572\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_2 to have 4 dimensions, but got array with shape (1, 1, 2)"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=15, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reloading Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
