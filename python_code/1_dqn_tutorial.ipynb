{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### Copyright 2021 The TF-Agents Authors."
   ],
   "metadata": {
    "id": "klGNgWREsvQv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
    "# you may not use this file except in compliance with the License.\r\n",
    "# You may obtain a copy of the License at\r\n",
    "#\r\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\r\n",
    "#\r\n",
    "# Unless required by applicable law or agreed to in writing, software\r\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
    "# See the License for the specific language governing permissions and\r\n",
    "# limitations under the License."
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:01.454344Z",
     "iopub.status.busy": "2021-06-09T12:08:01.453803Z",
     "iopub.status.idle": "2021-06-09T12:08:01.456292Z",
     "shell.execute_reply": "2021-06-09T12:08:01.455829Z"
    },
    "id": "nQnmcm0oI1Q-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train a Deep Q Network with TF-Agents\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/agents/docs/tutorials/1_dqn_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ],
   "metadata": {
    "id": "pmDI-h7cI0tI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n"
   ],
   "metadata": {
    "id": "lsaQlK8fFQqH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This example shows how to train a [DQN (Deep Q Networks)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)  agent on the Cartpole environment using the TF-Agents library.\r\n",
    "\r\n",
    "![Cartpole environment](https://raw.githubusercontent.com/tensorflow/agents/master/docs/tutorials/images/cartpole.png)\r\n",
    "\r\n",
    "It will walk you through all the components in a Reinforcement Learning (RL) pipeline for training, evaluation and data collection.\r\n",
    "\r\n",
    "\r\n",
    "To run this code live, click the 'Run in Google Colab' link above.\r\n"
   ],
   "metadata": {
    "id": "cKOCZlhUgXVK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "id": "1u9QVVsShC9X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you haven't installed the following dependencies, run:"
   ],
   "metadata": {
    "id": "kNrNXKI7bINP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# !sudo apt-get update\r\n",
    "# !sudo apt-get install -y xvfb ffmpeg\r\n",
    "# !pip install 'imageio==2.4.0'\r\n",
    "# !pip install pyvirtualdisplay\r\n",
    "# !pip install tf-agents"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:01.464785Z",
     "iopub.status.busy": "2021-06-09T12:08:01.464226Z",
     "iopub.status.idle": "2021-06-09T12:08:11.950927Z",
     "shell.execute_reply": "2021-06-09T12:08:11.951322Z"
    },
    "id": "KEHR2Ui-lo8O"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from __future__ import absolute_import, division, print_function\r\n",
    "\r\n",
    "import base64\r\n",
    "import imageio\r\n",
    "import IPython\r\n",
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import PIL.Image\r\n",
    "import pyvirtualdisplay\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "from tf_agents.agents.dqn import dqn_agent\r\n",
    "from tf_agents.environments import suite_gym\r\n",
    "from tf_agents.environments import tf_py_environment\r\n",
    "from tf_agents.eval import metric_utils\r\n",
    "from tf_agents.metrics import tf_metrics\r\n",
    "from tf_agents.networks import sequential\r\n",
    "from tf_agents.policies import random_tf_policy\r\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\r\n",
    "from tf_agents.trajectories import trajectory\r\n",
    "from tf_agents.specs import tensor_spec\r\n",
    "from tf_agents.utils import common"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:11.957178Z",
     "iopub.status.busy": "2021-06-09T12:08:11.956605Z",
     "iopub.status.idle": "2021-06-09T12:08:14.002999Z",
     "shell.execute_reply": "2021-06-09T12:08:14.002481Z"
    },
    "id": "sMitx5qSgJk1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# This line only work on ubuntu\r\n",
    "\r\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\r\n",
    "# display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.011610Z",
     "iopub.status.busy": "2021-06-09T12:08:14.006368Z",
     "iopub.status.idle": "2021-06-09T12:08:14.092921Z",
     "shell.execute_reply": "2021-06-09T12:08:14.093305Z"
    },
    "id": "J6HsdS5GbSjd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tf.version.VERSION"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.100145Z",
     "iopub.status.busy": "2021-06-09T12:08:14.099380Z",
     "iopub.status.idle": "2021-06-09T12:08:14.102243Z",
     "shell.execute_reply": "2021-06-09T12:08:14.102600Z"
    },
    "id": "NspmzG4nP3b9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters"
   ],
   "metadata": {
    "id": "LmC0NDhdLIKY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\r\n",
    "\r\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"} \r\n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\r\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\r\n",
    "\r\n",
    "batch_size = 64  # @param {type:\"integer\"}\r\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\r\n",
    "log_interval = 200  # @param {type:\"integer\"}\r\n",
    "\r\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\r\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.106935Z",
     "iopub.status.busy": "2021-06-09T12:08:14.106334Z",
     "iopub.status.idle": "2021-06-09T12:08:14.108518Z",
     "shell.execute_reply": "2021-06-09T12:08:14.108091Z"
    },
    "id": "HC1kNrOsLSIZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment\n",
    "\n",
    "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control.\n",
    "\n",
    "Load the CartPole environment from the OpenAI Gym suite. "
   ],
   "metadata": {
    "id": "VMsJC3DEgI0x"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "env_name = 'CartPole-v0'\r\n",
    "env = suite_gym.load(env_name)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.112114Z",
     "iopub.status.busy": "2021-06-09T12:08:14.111536Z",
     "iopub.status.idle": "2021-06-09T12:08:14.117412Z",
     "shell.execute_reply": "2021-06-09T12:08:14.116850Z"
    },
    "id": "pYEz-S9gEv2-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can render this environment to see how it looks. A free-swinging pole is attached to a cart.  The goal is to move the cart right or left in order to keep the pole pointing up."
   ],
   "metadata": {
    "id": "IIHYVBkuvPNw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#@test {\"skip\": true}\r\n",
    "env.reset()\r\n",
    "PIL.Image.fromarray(env.render())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x400 at 0x230095B5C88>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.121566Z",
     "iopub.status.busy": "2021-06-09T12:08:14.120981Z",
     "iopub.status.idle": "2021-06-09T12:08:14.484223Z",
     "shell.execute_reply": "2021-06-09T12:08:14.484699Z"
    },
    "id": "RlO7WIQHu_7D"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `environment.step` method takes an `action` in the environment and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n",
    "\n",
    "The `time_step_spec()` method returns the specification for the `TimeStep` tuple. Its `observation` attribute shows the shape of observations, the data types, and the ranges of allowed values. The `reward` attribute shows the same details for the reward.\n"
   ],
   "metadata": {
    "id": "B9_lskPOey18"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print('Observation Spec:')\r\n",
    "print(env.time_step_spec().observation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.490743Z",
     "iopub.status.busy": "2021-06-09T12:08:14.490094Z",
     "iopub.status.idle": "2021-06-09T12:08:14.492374Z",
     "shell.execute_reply": "2021-06-09T12:08:14.492822Z"
    },
    "id": "exDv57iHfwQV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print('Reward Spec:')\r\n",
    "print(env.time_step_spec().reward)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.496826Z",
     "iopub.status.busy": "2021-06-09T12:08:14.496229Z",
     "iopub.status.idle": "2021-06-09T12:08:14.498407Z",
     "shell.execute_reply": "2021-06-09T12:08:14.498735Z"
    },
    "id": "UxiSyCbBUQPi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `action_spec()` method returns the shape, data types, and allowed values of valid actions."
   ],
   "metadata": {
    "id": "b_lHcIcqUaqB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print('Action Spec:')\r\n",
    "print(env.action_spec())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.502299Z",
     "iopub.status.busy": "2021-06-09T12:08:14.501741Z",
     "iopub.status.idle": "2021-06-09T12:08:14.504359Z",
     "shell.execute_reply": "2021-06-09T12:08:14.503896Z"
    },
    "id": "bttJ4uxZUQBr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the Cartpole environment:\n",
    "\n",
    "-   `observation` is an array of 4 floats: \n",
    "    -   the position and velocity of the cart\n",
    "    -   the angular position and velocity of the pole \n",
    "-   `reward` is a scalar float value\n",
    "-   `action` is a scalar integer with only two possible values:\n",
    "    -   `0` — \"move left\"\n",
    "    -   `1` — \"move right\"\n"
   ],
   "metadata": {
    "id": "eJCgJnx3g0yY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "time_step = env.reset()\r\n",
    "print('Time step:')\r\n",
    "print(time_step)\r\n",
    "\r\n",
    "action = np.array(1, dtype=np.int32)\r\n",
    "\r\n",
    "next_time_step = env.step(action)\r\n",
    "print('Next time step:')\r\n",
    "print(next_time_step)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time step:\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.04539086,  0.04074453,  0.04076822, -0.0117895 ], dtype=float32),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(0)})\n",
      "Next time step:\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.04457597,  0.23525882,  0.04053244, -0.29133597], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1)})\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.511263Z",
     "iopub.status.busy": "2021-06-09T12:08:14.510707Z",
     "iopub.status.idle": "2021-06-09T12:08:14.512918Z",
     "shell.execute_reply": "2021-06-09T12:08:14.513258Z"
    },
    "id": "V2UGR5t_iZX-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usually two environments are instantiated: one for training and one for evaluation. "
   ],
   "metadata": {
    "id": "4JSc9GviWUBK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "train_py_env = suite_gym.load(env_name)\r\n",
    "eval_py_env = suite_gym.load(env_name)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.518219Z",
     "iopub.status.busy": "2021-06-09T12:08:14.517279Z",
     "iopub.status.idle": "2021-06-09T12:08:14.520519Z",
     "shell.execute_reply": "2021-06-09T12:08:14.520079Z"
    },
    "id": "N7brXNIGWXjC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Cartpole environment, like most environments, is written in pure Python. This is converted to TensorFlow using the `TFPyEnvironment` wrapper.\n",
    "\n",
    "The original environment's API uses Numpy arrays. The `TFPyEnvironment` converts these to `Tensors` to make it compatible with Tensorflow agents and policies.\n"
   ],
   "metadata": {
    "id": "zuUqXAVmecTU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\r\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.524730Z",
     "iopub.status.busy": "2021-06-09T12:08:14.524161Z",
     "iopub.status.idle": "2021-06-09T12:08:14.528917Z",
     "shell.execute_reply": "2021-06-09T12:08:14.528525Z"
    },
    "id": "Xp-Y4mD6eDhF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent\n",
    "\n",
    "The algorithm used to solve an RL problem is represented by an `Agent`. TF-Agents provides standard implementations of a variety of `Agents`, including:\n",
    "\n",
    "-   [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (used in this tutorial)\n",
    "-   [REINFORCE](https://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n",
    "-   [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n",
    "-   [TD3](https://arxiv.org/pdf/1802.09477.pdf)\n",
    "-   [PPO](https://arxiv.org/abs/1707.06347)\n",
    "-   [SAC](https://arxiv.org/abs/1801.01290).\n",
    "\n",
    "The DQN agent can be used in any environment which has a discrete action space.\n",
    "\n",
    "At the heart of a DQN Agent is a `QNetwork`, a neural network model that can learn to predict `QValues` (expected returns) for all actions, given an observation from the environment.\n",
    "\n",
    "We will use `tf_agents.networks.` to create a `QNetwork`. The network will consist of a sequence of `tf.keras.layers.Dense` layers, where the final layer will have 1 output for each possible action."
   ],
   "metadata": {
    "id": "E9lW_OZYFR8A"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "fc_layer_params = (100, 50)\r\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\r\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\r\n",
    "\r\n",
    "# Define a helper function to create Dense layers configured with the right\r\n",
    "# activation and kernel initializer.\r\n",
    "def dense_layer(num_units):\r\n",
    "  return tf.keras.layers.Dense(\r\n",
    "      num_units,\r\n",
    "      activation=tf.keras.activations.relu,\r\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\r\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\r\n",
    "\r\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\r\n",
    "# with `num_actions` units to generate one q_value per available action as\r\n",
    "# it's output.\r\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\r\n",
    "q_values_layer = tf.keras.layers.Dense(\r\n",
    "    num_actions,\r\n",
    "    activation=None,\r\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\r\n",
    "        minval=-0.03, maxval=0.03),\r\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\r\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.537481Z",
     "iopub.status.busy": "2021-06-09T12:08:14.534314Z",
     "iopub.status.idle": "2021-06-09T12:08:14.547005Z",
     "shell.execute_reply": "2021-06-09T12:08:14.546563Z"
    },
    "id": "TgkdEPg_muzV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now use `tf_agents.agents.dqn.dqn_agent` to instantiate a `DqnAgent`. In addition to the `time_step_spec`, `action_spec` and the QNetwork, the agent constructor also requires an optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
   ],
   "metadata": {
    "id": "z62u55hSmviJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\r\n",
    "\r\n",
    "train_step_counter = tf.Variable(0)\r\n",
    "\r\n",
    "agent = dqn_agent.DqnAgent(\r\n",
    "    train_env.time_step_spec(),\r\n",
    "    train_env.action_spec(),\r\n",
    "    q_network=q_net,\r\n",
    "    optimizer=optimizer,\r\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\r\n",
    "    train_step_counter=train_step_counter)\r\n",
    "\r\n",
    "agent.initialize()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.557560Z",
     "iopub.status.busy": "2021-06-09T12:08:14.556642Z",
     "iopub.status.idle": "2021-06-09T12:08:14.583406Z",
     "shell.execute_reply": "2021-06-09T12:08:14.582980Z"
    },
    "id": "jbY4yrjTEyc9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policies\n",
    "\n",
    "A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n",
    "\n",
    "In this tutorial:\n",
    "\n",
    "-   The desired outcome is keeping the pole balanced upright over the cart.\n",
    "-   The policy returns an action (left or right) for each `time_step` observation.\n",
    "\n",
    "Agents contain two policies: \n",
    "\n",
    "-   `agent.policy` — The main policy that is used for evaluation and deployment.\n",
    "-   `agent.collect_policy` — A second policy that is used for data collection.\n"
   ],
   "metadata": {
    "id": "I0KLrEPwkn5x"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "eval_policy = agent.policy\r\n",
    "collect_policy = agent.collect_policy"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.586763Z",
     "iopub.status.busy": "2021-06-09T12:08:14.586219Z",
     "iopub.status.idle": "2021-06-09T12:08:14.588277Z",
     "shell.execute_reply": "2021-06-09T12:08:14.587817Z"
    },
    "id": "BwY7StuMkuV4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Policies can be created independently of agents. For example, use `tf_agents.policies.random_tf_policy` to create a policy which will randomly select an action for each `time_step`."
   ],
   "metadata": {
    "id": "2Qs1Fl3dV0ae"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\r\n",
    "                                                train_env.action_spec())"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.591871Z",
     "iopub.status.busy": "2021-06-09T12:08:14.591259Z",
     "iopub.status.idle": "2021-06-09T12:08:14.593049Z",
     "shell.execute_reply": "2021-06-09T12:08:14.593386Z"
    },
    "id": "HE37-UCIrE69"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get an action from a policy, call the `policy.action(time_step)` method. The `time_step` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n",
    "\n",
    "-   `action` — the action to be taken (in this case, `0` or `1`)\n",
    "-   `state` — used for stateful (that is, RNN-based) policies\n",
    "-   `info` — auxiliary data, such as log probabilities of actions"
   ],
   "metadata": {
    "id": "dOlnlRRsUbxP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "example_environment = tf_py_environment.TFPyEnvironment(\r\n",
    "    suite_gym.load('CartPole-v0'))"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.597963Z",
     "iopub.status.busy": "2021-06-09T12:08:14.597402Z",
     "iopub.status.idle": "2021-06-09T12:08:14.716890Z",
     "shell.execute_reply": "2021-06-09T12:08:14.717263Z"
    },
    "id": "5gCcpXswVAxk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "time_step = example_environment.reset()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.721349Z",
     "iopub.status.busy": "2021-06-09T12:08:14.720749Z",
     "iopub.status.idle": "2021-06-09T12:08:14.722701Z",
     "shell.execute_reply": "2021-06-09T12:08:14.723022Z"
    },
    "id": "D4DHZtq3Ndis"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "random_policy.action(time_step)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0], dtype=int64)>, state=(), info=())"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.726419Z",
     "iopub.status.busy": "2021-06-09T12:08:14.725854Z",
     "iopub.status.idle": "2021-06-09T12:08:14.730052Z",
     "shell.execute_reply": "2021-06-09T12:08:14.730394Z"
    },
    "id": "PRFqAUzpNaAW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics and Evaluation\n",
    "\n",
    "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
    "\n",
    "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
   ],
   "metadata": {
    "id": "94rCXQtbUbXv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#@test {\"skip\": true}\r\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\r\n",
    "\r\n",
    "  total_return = 0.0\r\n",
    "  for _ in range(num_episodes):\r\n",
    "\r\n",
    "    time_step = environment.reset()\r\n",
    "    episode_return = 0.0\r\n",
    "\r\n",
    "    while not time_step.is_last():\r\n",
    "      action_step = policy.action(time_step)\r\n",
    "      time_step = environment.step(action_step.action)\r\n",
    "      episode_return += time_step.reward\r\n",
    "    total_return += episode_return\r\n",
    "\r\n",
    "  avg_return = total_return / num_episodes\r\n",
    "  return avg_return.numpy()[0]\r\n",
    "\r\n",
    "\r\n",
    "# See also the metrics module for standard implementations of different metrics.\r\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.735009Z",
     "iopub.status.busy": "2021-06-09T12:08:14.734447Z",
     "iopub.status.idle": "2021-06-09T12:08:14.736044Z",
     "shell.execute_reply": "2021-06-09T12:08:14.736380Z"
    },
    "id": "bitzHo5_UbXy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running this computation on the `random_policy` shows a baseline performance in the environment."
   ],
   "metadata": {
    "id": "_snCVvq5Z8lJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20.1"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.740249Z",
     "iopub.status.busy": "2021-06-09T12:08:14.739680Z",
     "iopub.status.idle": "2021-06-09T12:08:15.082571Z",
     "shell.execute_reply": "2021-06-09T12:08:15.082135Z"
    },
    "id": "9bgU6Q6BZ8Bp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
    "\n",
    "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required.\n"
   ],
   "metadata": {
    "id": "NLva6g2jdWgr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\r\n",
    "    data_spec=agent.collect_data_spec,\r\n",
    "    batch_size=train_env.batch_size,\r\n",
    "    max_length=replay_buffer_max_length)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.086440Z",
     "iopub.status.busy": "2021-06-09T12:08:15.085869Z",
     "iopub.status.idle": "2021-06-09T12:08:15.095564Z",
     "shell.execute_reply": "2021-06-09T12:08:15.095099Z"
    },
    "id": "vX2zGUWJGWAl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
   ],
   "metadata": {
    "id": "ZGNTDJpZs4NN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "agent.collect_data_spec"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Trajectory(\n",
       "{'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0, dtype=int64), maximum=array(1, dtype=int64)),\n",
       " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
       " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
       " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "      dtype=float32)),\n",
       " 'policy_info': (),\n",
       " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
       " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.100519Z",
     "iopub.status.busy": "2021-06-09T12:08:15.099957Z",
     "iopub.status.idle": "2021-06-09T12:08:15.102769Z",
     "shell.execute_reply": "2021-06-09T12:08:15.102379Z"
    },
    "id": "_IZ-3HcqgE1z"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "agent.collect_data_spec._fields"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('step_type',\n",
       " 'observation',\n",
       " 'action',\n",
       " 'policy_info',\n",
       " 'next_step_type',\n",
       " 'reward',\n",
       " 'discount')"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.106194Z",
     "iopub.status.busy": "2021-06-09T12:08:15.105617Z",
     "iopub.status.idle": "2021-06-09T12:08:15.108347Z",
     "shell.execute_reply": "2021-06-09T12:08:15.107944Z"
    },
    "id": "sy6g1tGcfRlw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Collection\n",
    "\n",
    "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
   ],
   "metadata": {
    "id": "rVD5nQ9ZGo8_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "#@test {\"skip\": true}\r\n",
    "def collect_step(environment, policy, buffer):\r\n",
    "  time_step = environment.current_time_step()\r\n",
    "  action_step = policy.action(time_step)\r\n",
    "  next_time_step = environment.step(action_step.action)\r\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\r\n",
    "\r\n",
    "  # Add trajectory to the replay buffer\r\n",
    "  buffer.add_batch(traj)\r\n",
    "\r\n",
    "def collect_data(env, policy, buffer, steps):\r\n",
    "  for _ in range(steps):\r\n",
    "    collect_step(env, policy, buffer)\r\n",
    "\r\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\r\n",
    "\r\n",
    "# This loop is so common in RL, that we provide standard implementations. \r\n",
    "# For more details see tutorial 4 or the drivers module.\r\n",
    "# https://github.com/tensorflow/agents/blob/master/docs/tutorials/4_drivers_tutorial.ipynb \r\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.113897Z",
     "iopub.status.busy": "2021-06-09T12:08:15.112829Z",
     "iopub.status.idle": "2021-06-09T12:08:15.327852Z",
     "shell.execute_reply": "2021-06-09T12:08:15.327409Z"
    },
    "id": "wr1KSAEGG4h9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The replay buffer is now a collection of Trajectories."
   ],
   "metadata": {
    "id": "84z5pQJdoKxo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "# For the curious:\r\n",
    "# Uncomment to peel one of these off and inspect it.\r\n",
    "# iter(replay_buffer.as_dataset()).next()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.330914Z",
     "iopub.status.busy": "2021-06-09T12:08:15.330365Z",
     "iopub.status.idle": "2021-06-09T12:08:15.332044Z",
     "shell.execute_reply": "2021-06-09T12:08:15.332385Z"
    },
    "id": "4wZnLu2ViO4E"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
    "\n",
    "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
    "\n",
    "This dataset is also optimized by running parallel calls and prefetching data."
   ],
   "metadata": {
    "id": "TujU-PMUsKjS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\r\n",
    "dataset = replay_buffer.as_dataset(\r\n",
    "    num_parallel_calls=3, \r\n",
    "    sample_batch_size=batch_size, \r\n",
    "    num_steps=2).prefetch(3)\r\n",
    "\r\n",
    "\r\n",
    "dataset"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\counter.py:66: scan (from tensorflow.python.data.experimental.ops.scan_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.scan(...) instead\n",
      "WARNING:tensorflow:From C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (Trajectory(\n",
       "{action: (64, 2),\n",
       " discount: (64, 2),\n",
       " next_step_type: (64, 2),\n",
       " observation: (64, 2, 4),\n",
       " policy_info: (),\n",
       " reward: (64, 2),\n",
       " step_type: (64, 2)}), BufferInfo(ids=(64, 2), probabilities=(64,))), types: (Trajectory(\n",
       "{action: tf.int64,\n",
       " discount: tf.float32,\n",
       " next_step_type: tf.int32,\n",
       " observation: tf.float32,\n",
       " policy_info: (),\n",
       " reward: tf.float32,\n",
       " step_type: tf.int32}), BufferInfo(ids=tf.int64, probabilities=tf.float32))>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.335992Z",
     "iopub.status.busy": "2021-06-09T12:08:15.335415Z",
     "iopub.status.idle": "2021-06-09T12:08:15.917058Z",
     "shell.execute_reply": "2021-06-09T12:08:15.917395Z"
    },
    "id": "ba7bilizt_qW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "iterator = iter(dataset)\r\n",
    "print(iterator)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x000002300ADE3B88>\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.920917Z",
     "iopub.status.busy": "2021-06-09T12:08:15.920327Z",
     "iopub.status.idle": "2021-06-09T12:08:15.978263Z",
     "shell.execute_reply": "2021-06-09T12:08:15.978640Z"
    },
    "id": "K13AST-2ppOq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# For the curious:\r\n",
    "# Uncomment to see what the dataset iterator is feeding to the agent.\r\n",
    "# Compare this representation of replay data \r\n",
    "# to the collection of individual trajectories shown earlier.\r\n",
    "\r\n",
    "# iterator.next()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.981611Z",
     "iopub.status.busy": "2021-06-09T12:08:15.981053Z",
     "iopub.status.idle": "2021-06-09T12:08:15.982722Z",
     "shell.execute_reply": "2021-06-09T12:08:15.983047Z"
    },
    "id": "Th5w5Sff0b16"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the agent\n",
    "\n",
    "Two things must happen during the training loop:\n",
    "\n",
    "-   collect data from the environment\n",
    "-   use that data to train the agent's neural network(s)\n",
    "\n",
    "This example also periodicially evaluates the policy and prints the current score.\n",
    "\n",
    "The following will take ~5 minutes to run."
   ],
   "metadata": {
    "id": "hBc9lj9VWWtZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "#@test {\"skip\": true}\r\n",
    "try:\r\n",
    "  %%time\r\n",
    "except:\r\n",
    "  pass\r\n",
    "\r\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\r\n",
    "agent.train = common.function(agent.train)\r\n",
    "\r\n",
    "# Reset the train step\r\n",
    "agent.train_step_counter.assign(0)\r\n",
    "\r\n",
    "# Evaluate the agent's policy once before training.\r\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\r\n",
    "returns = [avg_return]\r\n",
    "\r\n",
    "for _ in range(num_iterations):\r\n",
    "\r\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\r\n",
    "    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\r\n",
    "\r\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\r\n",
    "    experience, unused_info = next(iterator)\r\n",
    "    train_loss = agent.train(experience).loss\r\n",
    "\r\n",
    "    step = agent.train_step_counter.numpy()\r\n",
    "\r\n",
    "    if step % log_interval == 0:\r\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\r\n",
    "\r\n",
    "    if step % eval_interval == 0:\r\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\r\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\r\n",
    "        returns.append(avg_return)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "step = 200: loss = 22.344623565673828\n",
      "step = 400: loss = 176.30027770996094\n",
      "step = 600: loss = 41.295310974121094\n",
      "step = 800: loss = 14.57300853729248\n",
      "step = 1000: loss = 50.89461898803711\n",
      "step = 1000: Average Return = 106.0999984741211\n",
      "step = 1200: loss = 54.338802337646484\n",
      "step = 1400: loss = 79.97113800048828\n",
      "step = 1600: loss = 71.2186508178711\n",
      "step = 1800: loss = 358.6416931152344\n",
      "step = 2000: loss = 53.77468490600586\n",
      "step = 2000: Average Return = 94.30000305175781\n",
      "step = 2200: loss = 27.484397888183594\n",
      "step = 2400: loss = 21.076900482177734\n",
      "step = 2600: loss = 11.032693862915039\n",
      "step = 2800: loss = 11.880784034729004\n",
      "step = 3000: loss = 17.327796936035156\n",
      "step = 3000: Average Return = 184.0\n",
      "step = 3200: loss = 10.160411834716797\n",
      "step = 3400: loss = 12.322681427001953\n",
      "step = 3600: loss = 11.871136665344238\n",
      "step = 3800: loss = 23.31613540649414\n",
      "step = 4000: loss = 21.96761131286621\n",
      "step = 4000: Average Return = 200.0\n",
      "step = 4200: loss = 38.09566879272461\n",
      "step = 4400: loss = 87.1005859375\n",
      "step = 4600: loss = 384.00445556640625\n",
      "step = 4800: loss = 643.830322265625\n",
      "step = 5000: loss = 1480.86328125\n",
      "step = 5000: Average Return = 200.0\n",
      "step = 5200: loss = 3327.703857421875\n",
      "step = 5400: loss = 5187.9912109375\n",
      "step = 5600: loss = 13574.78515625\n",
      "step = 5800: loss = 33757.37890625\n",
      "step = 6000: loss = 51205.203125\n",
      "step = 6000: Average Return = 200.0\n",
      "step = 6200: loss = 69273.1796875\n",
      "step = 6400: loss = 207267.109375\n",
      "step = 6600: loss = 200726.90625\n",
      "step = 6800: loss = 208607.609375\n",
      "step = 7000: loss = 9994346.0\n",
      "step = 7000: Average Return = 200.0\n",
      "step = 7200: loss = 329496.59375\n",
      "step = 7400: loss = 373242.375\n",
      "step = 7600: loss = 591310.0625\n",
      "step = 7800: loss = 363599.65625\n",
      "step = 8000: loss = 544562.4375\n",
      "step = 8000: Average Return = 200.0\n",
      "step = 8200: loss = 615730.0\n",
      "step = 8400: loss = 589894.25\n",
      "step = 8600: loss = 543304.875\n",
      "step = 8800: loss = 747215.0\n",
      "step = 9000: loss = 1313056.0\n",
      "step = 9000: Average Return = 200.0\n",
      "step = 9200: loss = 1353857.0\n",
      "step = 9400: loss = 3261958.0\n",
      "step = 9600: loss = 3008901.75\n",
      "step = 9800: loss = 2732363.75\n",
      "step = 10000: loss = 1303230.25\n",
      "step = 10000: Average Return = 200.0\n",
      "step = 10200: loss = 5187998.5\n",
      "step = 10400: loss = 4148439.5\n",
      "step = 10600: loss = 4281526.0\n",
      "step = 10800: loss = 6586080.5\n",
      "step = 11000: loss = 5298908.5\n",
      "step = 11000: Average Return = 200.0\n",
      "step = 11200: loss = 5318860.5\n",
      "step = 11400: loss = 8499964.0\n",
      "step = 11600: loss = 10538574.0\n",
      "step = 11800: loss = 5445987.0\n",
      "step = 12000: loss = 7169088.0\n",
      "step = 12000: Average Return = 200.0\n",
      "step = 12200: loss = 15277833.0\n",
      "step = 12400: loss = 289039680.0\n",
      "step = 12600: loss = 7666541.0\n",
      "step = 12800: loss = 12833396.0\n",
      "step = 13000: loss = 7866883.0\n",
      "step = 13000: Average Return = 200.0\n",
      "step = 13200: loss = 10362794.0\n",
      "step = 13400: loss = 72080664.0\n",
      "step = 13600: loss = 11649914.0\n",
      "step = 13800: loss = 18870560.0\n",
      "step = 14000: loss = 29325476.0\n",
      "step = 14000: Average Return = 200.0\n",
      "step = 14200: loss = 15131633.0\n",
      "step = 14400: loss = 129006328.0\n",
      "step = 14600: loss = 22704520.0\n",
      "step = 14800: loss = 30098290.0\n",
      "step = 15000: loss = 2322630400.0\n",
      "step = 15000: Average Return = 200.0\n",
      "step = 15200: loss = 32828606.0\n",
      "step = 15400: loss = 32405268.0\n",
      "step = 15600: loss = 55741464.0\n",
      "step = 15800: loss = 61714028.0\n",
      "step = 16000: loss = 47178036.0\n",
      "step = 16000: Average Return = 200.0\n",
      "step = 16200: loss = 90670336.0\n",
      "step = 16400: loss = 247625040.0\n",
      "step = 16600: loss = 127746728.0\n",
      "step = 16800: loss = 229475392.0\n",
      "step = 17000: loss = 2854635264.0\n",
      "step = 17000: Average Return = 143.5\n",
      "step = 17200: loss = 379881760.0\n",
      "step = 17400: loss = 467856448.0\n",
      "step = 17600: loss = 9832294400.0\n",
      "step = 17800: loss = 781599040.0\n",
      "step = 18000: loss = 659896832.0\n",
      "step = 18000: Average Return = 200.0\n",
      "step = 18200: loss = 878377472.0\n",
      "step = 18400: loss = 1114564608.0\n",
      "step = 18600: loss = 19797483520.0\n",
      "step = 18800: loss = 36553674752.0\n",
      "step = 19000: loss = 28682428416.0\n",
      "step = 19000: Average Return = 13.600000381469727\n",
      "step = 19200: loss = 68535283712.0\n",
      "step = 19400: loss = 21083248640.0\n",
      "step = 19600: loss = 34733092864.0\n",
      "step = 19800: loss = 14133063680.0\n",
      "step = 20000: loss = 2155392256.0\n",
      "step = 20000: Average Return = 71.9000015258789\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.990870Z",
     "iopub.status.busy": "2021-06-09T12:08:15.990298Z",
     "iopub.status.idle": "2021-06-09T12:12:24.897936Z",
     "shell.execute_reply": "2021-06-09T12:12:24.898275Z"
    },
    "id": "0pTbJ3PeyF-u"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization\n"
   ],
   "metadata": {
    "id": "68jNcA_TiJDq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plots\n",
    "\n",
    "Use `matplotlib.pyplot` to chart how the policy improved during training.\n",
    "\n",
    "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
   ],
   "metadata": {
    "id": "aO-LWCdbbOIC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "#@test {\"skip\": true}\r\n",
    "\r\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\r\n",
    "plt.plot(iterations, returns)\r\n",
    "plt.ylabel('Average Return')\r\n",
    "plt.xlabel('Iterations')\r\n",
    "plt.ylim(top=250)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.1849997997283932, 250)"
      ]
     },
     "metadata": {},
     "execution_count": 33
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"265.995469pt\" version=\"1.1\" viewBox=\"0 0 389.653693 265.995469\" width=\"389.653693pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 265.995469 \r\nL 389.653693 265.995469 \r\nL 389.653693 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 46.965625 228.439219 \r\nL 381.765625 228.439219 \r\nL 381.765625 10.999219 \r\nL 46.965625 10.999219 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mdb1d38169f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.183807\" xlink:href=\"#mdb1d38169f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(59.002557 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"100.229261\" xlink:href=\"#mdb1d38169f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 2500 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(87.504261 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"138.274716\" xlink:href=\"#mdb1d38169f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 5000 -->\r\n      <g transform=\"translate(125.549716 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"176.32017\" xlink:href=\"#mdb1d38169f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 7500 -->\r\n      <defs>\r\n       <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n      </defs>\r\n      <g transform=\"translate(163.59517 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"214.365625\" xlink:href=\"#mdb1d38169f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 10000 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(198.459375 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"252.41108\" xlink:href=\"#mdb1d38169f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 12500 -->\r\n      <g transform=\"translate(236.50483 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"290.456534\" xlink:href=\"#mdb1d38169f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 15000 -->\r\n      <g transform=\"translate(274.550284 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"328.501989\" xlink:href=\"#mdb1d38169f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 17500 -->\r\n      <g transform=\"translate(312.595739 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"366.547443\" xlink:href=\"#mdb1d38169f\" y=\"228.439219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 20000 -->\r\n      <g transform=\"translate(350.641193 243.037656)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_10\">\r\n     <!-- Iterations -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-73\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(190.552344 256.715781)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-73\"/>\r\n      <use x=\"29.492188\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"68.701172\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"130.224609\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"171.337891\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"232.617188\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"271.826172\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"299.609375\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"360.791016\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"424.169922\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_10\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mef4ff6261c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#mef4ff6261c\" y=\"185.080038\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(27.240625 188.879257)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#mef4ff6261c\" y=\"141.559834\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(20.878125 145.359052)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#mef4ff6261c\" y=\"98.039629\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(20.878125 101.838847)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#mef4ff6261c\" y=\"54.519424\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(20.878125 58.318642)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#mef4ff6261c\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- Average Return -->\r\n     <defs>\r\n      <path d=\"M 34.1875 63.1875 \r\nL 20.796875 26.90625 \r\nL 47.609375 26.90625 \r\nz\r\nM 28.609375 72.90625 \r\nL 39.796875 72.90625 \r\nL 67.578125 0 \r\nL 57.328125 0 \r\nL 50.6875 18.703125 \r\nL 17.828125 18.703125 \r\nL 11.1875 0 \r\nL 0.78125 0 \r\nz\r\n\" id=\"DejaVuSans-65\"/>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n      <path d=\"M 44.390625 34.1875 \r\nQ 47.5625 33.109375 50.5625 29.59375 \r\nQ 53.5625 26.078125 56.59375 19.921875 \r\nL 66.609375 0 \r\nL 56 0 \r\nL 46.6875 18.703125 \r\nQ 43.0625 26.03125 39.671875 28.421875 \r\nQ 36.28125 30.8125 30.421875 30.8125 \r\nL 19.671875 30.8125 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nL 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.578125 72.90625 50.734375 67.671875 \r\nQ 56.890625 62.453125 56.890625 51.90625 \r\nQ 56.890625 45.015625 53.6875 40.46875 \r\nQ 50.484375 35.9375 44.390625 34.1875 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 38.921875 \r\nL 32.078125 38.921875 \r\nQ 39.203125 38.921875 42.84375 42.21875 \r\nQ 46.484375 45.515625 46.484375 51.90625 \r\nQ 46.484375 58.296875 42.84375 61.546875 \r\nQ 39.203125 64.796875 32.078125 64.796875 \r\nz\r\n\" id=\"DejaVuSans-82\"/>\r\n      <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 159.030937)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"68.330078\" xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"127.509766\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"189.033203\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"230.146484\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"291.425781\" xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"354.902344\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"416.425781\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"448.212891\" xlink:href=\"#DejaVuSans-82\"/>\r\n      <use x=\"517.632812\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"579.15625\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"618.365234\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"681.744141\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"722.841797\" xlink:href=\"#DejaVuSans-110\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#pd5fcceec8b)\" d=\"M 62.183807 220.157324 \r\nL 77.401989 136.25037 \r\nL 92.62017 146.521134 \r\nL 107.838352 68.445889 \r\nL 123.056534 54.519424 \r\nL 138.274716 54.519424 \r\nL 153.492898 54.519424 \r\nL 168.71108 54.519424 \r\nL 183.929261 54.519424 \r\nL 199.147443 54.519424 \r\nL 214.365625 54.519424 \r\nL 229.583807 54.519424 \r\nL 244.801989 54.519424 \r\nL 260.02017 54.519424 \r\nL 275.238352 54.519424 \r\nL 290.456534 54.519424 \r\nL 305.674716 54.519424 \r\nL 320.892898 103.697255 \r\nL 336.11108 54.519424 \r\nL 351.329261 216.762747 \r\nL 366.547443 166.018187 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 46.965625 228.439219 \r\nL 46.965625 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 381.765625 228.439219 \r\nL 381.765625 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 46.965625 228.439219 \r\nL 381.765625 228.439219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 46.965625 10.999219 \r\nL 381.765625 10.999219 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pd5fcceec8b\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"46.965625\" y=\"10.999219\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:12:24.915349Z",
     "iopub.status.busy": "2021-06-09T12:12:24.914806Z",
     "iopub.status.idle": "2021-06-09T12:12:25.019504Z",
     "shell.execute_reply": "2021-06-09T12:12:25.019844Z"
    },
    "id": "NxtL1mbOYCVO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Videos"
   ],
   "metadata": {
    "id": "M7-XpPP99Cy7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Charts are nice. But more exciting is seeing an agent actually performing a task in an environment. \n",
    "\n",
    "First, create a function to embed videos in the notebook."
   ],
   "metadata": {
    "id": "9pGfGxSH32gn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "def embed_mp4(filename):\r\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\r\n",
    "  video = open(filename,'rb').read()\r\n",
    "  b64 = base64.b64encode(video)\r\n",
    "  tag = '''\r\n",
    "  <video width=\"640\" height=\"480\" controls>\r\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\r\n",
    "  Your browser does not support the video tag.\r\n",
    "  </video>'''.format(b64.decode())\r\n",
    "\r\n",
    "  return IPython.display.HTML(tag)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:12:25.024404Z",
     "iopub.status.busy": "2021-06-09T12:12:25.023843Z",
     "iopub.status.idle": "2021-06-09T12:12:25.025897Z",
     "shell.execute_reply": "2021-06-09T12:12:25.025498Z"
    },
    "id": "ULaGr8pvOKbl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
   ],
   "metadata": {
    "id": "9c_PH-pX4Pr5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\r\n",
    "  filename = filename + \".mp4\"\r\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\r\n",
    "    for _ in range(num_episodes):\r\n",
    "      time_step = eval_env.reset()\r\n",
    "      video.append_data(eval_py_env.render())\r\n",
    "      while not time_step.is_last():\r\n",
    "        action_step = policy.action(time_step)\r\n",
    "        time_step = eval_env.step(action_step.action)\r\n",
    "        video.append_data(eval_py_env.render())\r\n",
    "  return embed_mp4(filename)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "create_policy_eval_video(agent.policy, \"trained-agent\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NeedDownloadError",
     "evalue": "Need ffmpeg exe. You can obtain it with either:\n  - install using conda: conda install ffmpeg -c conda-forge\n  - download using the command: imageio_download_bin ffmpeg\n  - download by calling (in Python): imageio.plugins.ffmpeg.download()\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNeedDownloadError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-1b5599f76f3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mcreate_policy_eval_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"trained-agent\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-1b5599f76f3d>\u001b[0m in \u001b[0;36mcreate_policy_eval_video\u001b[1;34m(policy, filename, num_episodes, fps)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_policy_eval_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".mp4\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[1;32mwith\u001b[0m \u001b[0mimageio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mvideo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m       \u001b[0mtime_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_env\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageio\\core\\functions.py\u001b[0m in \u001b[0;36mget_writer\u001b[1;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# Return its writer object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageio\\core\\format.py\u001b[0m in \u001b[0;36mget_writer\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    186\u001b[0m                 \u001b[1;34m\"Format %s cannot write in mode %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselect_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             )\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcan_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageio\\core\\format.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, format, request)\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[1;31m# Open the reader/writer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageio\\plugins\\ffmpeg.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, fps, codec, bitrate, pixelformat, ffmpeg_params, input_params, output_params, ffmpeg_log_level, quality, macro_block_size)\u001b[0m\n\u001b[0;32m    674\u001b[0m             \u001b[0mmacro_block_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m         ):\n\u001b[1;32m--> 676\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_exe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m             \u001b[1;31m# Get local filename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_local_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageio\\plugins\\ffmpeg.py\u001b[0m in \u001b[0;36m_get_exe\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    658\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_exe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 660\u001b[1;33m             \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exe\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mget_exe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    661\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\imageio\\plugins\\ffmpeg.py\u001b[0m in \u001b[0;36mget_exe\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;31m# Nothing was found so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     raise NeedDownloadError(\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[1;34m\"Need ffmpeg exe. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[1;34m\"You can obtain it with either:\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;34m\"  - install using conda: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNeedDownloadError\u001b[0m: Need ffmpeg exe. You can obtain it with either:\n  - install using conda: conda install ffmpeg -c conda-forge\n  - download using the command: imageio_download_bin ffmpeg\n  - download by calling (in Python): imageio.plugins.ffmpeg.download()\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:12:25.031028Z",
     "iopub.status.busy": "2021-06-09T12:12:25.030437Z",
     "iopub.status.idle": "2021-06-09T12:12:33.395239Z",
     "shell.execute_reply": "2021-06-09T12:12:33.395676Z"
    },
    "id": "owOVWB158NlF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For fun, compare the trained agent (above) to an agent moving randomly. (It does not do as well.)"
   ],
   "metadata": {
    "id": "povaAOcZygLw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "create_policy_eval_video(random_policy, \"random-agent\")"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:12:33.400440Z",
     "iopub.status.busy": "2021-06-09T12:12:33.399795Z",
     "iopub.status.idle": "2021-06-09T12:12:34.269219Z",
     "shell.execute_reply": "2021-06-09T12:12:34.269579Z"
    },
    "id": "pJZIdC37yNH4"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQN Tutorial.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "interpreter": {
   "hash": "e48d23369a553edc96f1373b6255b5687d82d68cd08867622b2500e338930542"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}