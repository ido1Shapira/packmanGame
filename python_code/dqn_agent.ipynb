{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### Copyright 2021 The TF-Agents Authors."
   ],
   "metadata": {
    "id": "klGNgWREsvQv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
    "# you may not use this file except in compliance with the License.\r\n",
    "# You may obtain a copy of the License at\r\n",
    "#\r\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\r\n",
    "#\r\n",
    "# Unless required by applicable law or agreed to in writing, software\r\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
    "# See the License for the specific language governing permissions and\r\n",
    "# limitations under the License."
   ],
   "outputs": [],
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:01.454344Z",
     "iopub.status.busy": "2021-06-09T12:08:01.453803Z",
     "iopub.status.idle": "2021-06-09T12:08:01.456292Z",
     "shell.execute_reply": "2021-06-09T12:08:01.455829Z"
    },
    "id": "nQnmcm0oI1Q-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train a Deep Q Network with TF-Agents\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/agents/blob/master/docs/tutorials/1_dqn_tutorial.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/agents/docs/tutorials/1_dqn_tutorial.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ],
   "metadata": {
    "id": "pmDI-h7cI0tI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n"
   ],
   "metadata": {
    "id": "lsaQlK8fFQqH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This example shows how to train a [DQN (Deep Q Networks)](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)  agent on the Cartpole environment using the TF-Agents library.\r\n",
    "\r\n",
    "![Cartpole environment](https://raw.githubusercontent.com/tensorflow/agents/master/docs/tutorials/images/cartpole.png)\r\n",
    "\r\n",
    "It will walk you through all the components in a Reinforcement Learning (RL) pipeline for training, evaluation and data collection.\r\n",
    "\r\n",
    "\r\n",
    "To run this code live, click the 'Run in Google Colab' link above.\r\n"
   ],
   "metadata": {
    "id": "cKOCZlhUgXVK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "id": "1u9QVVsShC9X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you haven't installed the following dependencies, run:"
   ],
   "metadata": {
    "id": "kNrNXKI7bINP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# !sudo apt-get update\r\n",
    "# !sudo apt-get install -y xvfb ffmpeg\r\n",
    "# !pip install 'imageio==2.4.0'\r\n",
    "# !pip install pyvirtualdisplay\r\n",
    "# !pip install tf-agents"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:01.464785Z",
     "iopub.status.busy": "2021-06-09T12:08:01.464226Z",
     "iopub.status.idle": "2021-06-09T12:08:11.950927Z",
     "shell.execute_reply": "2021-06-09T12:08:11.951322Z"
    },
    "id": "KEHR2Ui-lo8O"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from __future__ import absolute_import, division, print_function\r\n",
    "\r\n",
    "import base64\r\n",
    "import imageio\r\n",
    "import IPython\r\n",
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import PIL.Image\r\n",
    "import pyvirtualdisplay\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "from tf_agents.agents.dqn import dqn_agent\r\n",
    "from tf_agents.environments import suite_gym\r\n",
    "from tf_agents.environments import tf_py_environment\r\n",
    "from tf_agents.eval import metric_utils\r\n",
    "from tf_agents.metrics import tf_metrics\r\n",
    "from tf_agents.networks import sequential\r\n",
    "from tf_agents.policies import random_tf_policy\r\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\r\n",
    "from tf_agents.trajectories import trajectory\r\n",
    "from tf_agents.specs import tensor_spec\r\n",
    "from tf_agents.utils import common"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:11.957178Z",
     "iopub.status.busy": "2021-06-09T12:08:11.956605Z",
     "iopub.status.idle": "2021-06-09T12:08:14.002999Z",
     "shell.execute_reply": "2021-06-09T12:08:14.002481Z"
    },
    "id": "sMitx5qSgJk1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# This line only work on ubuntu\r\n",
    "\r\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\r\n",
    "# display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.011610Z",
     "iopub.status.busy": "2021-06-09T12:08:14.006368Z",
     "iopub.status.idle": "2021-06-09T12:08:14.092921Z",
     "shell.execute_reply": "2021-06-09T12:08:14.093305Z"
    },
    "id": "J6HsdS5GbSjd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tf.version.VERSION"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.100145Z",
     "iopub.status.busy": "2021-06-09T12:08:14.099380Z",
     "iopub.status.idle": "2021-06-09T12:08:14.102243Z",
     "shell.execute_reply": "2021-06-09T12:08:14.102600Z"
    },
    "id": "NspmzG4nP3b9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters"
   ],
   "metadata": {
    "id": "LmC0NDhdLIKY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\r\n",
    "\r\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"} \r\n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\r\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\r\n",
    "\r\n",
    "batch_size = 64  # @param {type:\"integer\"}\r\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\r\n",
    "log_interval = 200  # @param {type:\"integer\"}\r\n",
    "\r\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\r\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.106935Z",
     "iopub.status.busy": "2021-06-09T12:08:14.106334Z",
     "iopub.status.idle": "2021-06-09T12:08:14.108518Z",
     "shell.execute_reply": "2021-06-09T12:08:14.108091Z"
    },
    "id": "HC1kNrOsLSIZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment\r\n",
    "\r\n",
    "In Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control.\r\n",
    "\r\n",
    "Load the CartPole environment from the OpenAI Gym suite. "
   ],
   "metadata": {
    "id": "VMsJC3DEgI0x"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "env_name = 'gym_packman:Packman-v0'\r\n",
    "env = suite_gym.load(env_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\gym\\logger.py:34: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize(\"%s: %s\" % (\"WARN\", msg % args), \"yellow\"))\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.112114Z",
     "iopub.status.busy": "2021-06-09T12:08:14.111536Z",
     "iopub.status.idle": "2021-06-09T12:08:14.117412Z",
     "shell.execute_reply": "2021-06-09T12:08:14.116850Z"
    },
    "id": "pYEz-S9gEv2-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can render this environment to see how it looks. A free-swinging pole is attached to a cart.  The goal is to move the cart right or left in order to keep the pole pointing up."
   ],
   "metadata": {
    "id": "IIHYVBkuvPNw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#@test {\"skip\": true}\r\n",
    "env.reset()\r\n",
    "plt.imshow(env.render(mode='rgb_array'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e3ed1efc88>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 245.2025 248.518125\" width=\"245.2025pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 245.2025 248.518125 \r\nL 245.2025 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 238.0025 224.64 \r\nL 238.0025 7.2 \r\nL 20.5625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#pfecf91b3b2)\">\r\n    <image height=\"218\" id=\"image7b7075427c\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"20.5625\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAAABHNCSVQICAgIfAhkiAAAAwhJREFUeJzt3EFKxFAQRdGKuK82K0+7snbmXLCuBM9ZQBEIlz97x8y8Blj19tcfAP+B0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDwPva5cfa5fv43Dl7va6Vu+fHuXL3Vpb+mRcNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQLHzLw2Dq8tNR2WmrZs/bNZWNc6l9aqtnjRICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoPA2jjPPFauwq6l0R8vGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoE3v/6A35sY6Voa7FraVHpdt+74WYra140CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAjcbwWLe61VzdxusWqDFw0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAsfMvFYuWz5i0fW8Vu6ex7ly14sGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBoG9Fawtj99fP7qev35yZvYWlZi9lbXPnbNeNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgsDfOszWewtqAjH82xnngzoQGAaFBQGgQEBoEhAYBoUFAaBAQGgSEBgGhQUBoEBAaBIQGAaFBQGgQEBoEhAYBoUFAaBAQGgT2VrCAb140CAgNAkKDgNAgIDQICA0CQoOA0CAgNAgIDQJCg4DQICA0CAgNAkKDgNAgIDQICA0CQoOA0CAgNAh8AT5LIq1NGcQeAAAAAElFTkSuQmCC\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m60bed881f8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"31.4345\" xlink:href=\"#m60bed881f8\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(28.25325 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"74.9225\" xlink:href=\"#m60bed881f8\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(71.74125 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"118.4105\" xlink:href=\"#m60bed881f8\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(115.22925 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"161.8985\" xlink:href=\"#m60bed881f8\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(158.71725 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"205.3865\" xlink:href=\"#m60bed881f8\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(202.20525 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m31a11337d4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m31a11337d4\" y=\"18.072\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(7.2 21.871219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m31a11337d4\" y=\"61.56\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(7.2 65.359219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m31a11337d4\" y=\"105.048\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(7.2 108.847219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m31a11337d4\" y=\"148.536\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 6 -->\r\n      <g transform=\"translate(7.2 152.335219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m31a11337d4\" y=\"192.024\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 8 -->\r\n      <g transform=\"translate(7.2 195.823219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 20.5625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 238.0025 224.64 \r\nL 238.0025 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 238.0025 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 20.5625 7.2 \r\nL 238.0025 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pfecf91b3b2\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"20.5625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.121566Z",
     "iopub.status.busy": "2021-06-09T12:08:14.120981Z",
     "iopub.status.idle": "2021-06-09T12:08:14.484223Z",
     "shell.execute_reply": "2021-06-09T12:08:14.484699Z"
    },
    "id": "RlO7WIQHu_7D"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `environment.step` method takes an `action` in the environment and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n",
    "\n",
    "The `time_step_spec()` method returns the specification for the `TimeStep` tuple. Its `observation` attribute shows the shape of observations, the data types, and the ranges of allowed values. The `reward` attribute shows the same details for the reward.\n"
   ],
   "metadata": {
    "id": "B9_lskPOey18"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print('Observation Spec:')\r\n",
    "print(env.time_step_spec().observation)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(10, 10, 6), dtype=dtype('float32'), name='observation', minimum=0.0, maximum=1.0)\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.490743Z",
     "iopub.status.busy": "2021-06-09T12:08:14.490094Z",
     "iopub.status.idle": "2021-06-09T12:08:14.492374Z",
     "shell.execute_reply": "2021-06-09T12:08:14.492822Z"
    },
    "id": "exDv57iHfwQV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print('Reward Spec:')\r\n",
    "print(env.time_step_spec().reward)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.496826Z",
     "iopub.status.busy": "2021-06-09T12:08:14.496229Z",
     "iopub.status.idle": "2021-06-09T12:08:14.498407Z",
     "shell.execute_reply": "2021-06-09T12:08:14.498735Z"
    },
    "id": "UxiSyCbBUQPi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `action_spec()` method returns the shape, data types, and allowed values of valid actions."
   ],
   "metadata": {
    "id": "b_lHcIcqUaqB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print('Action Spec:')\r\n",
    "print(env.action_spec())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=4)\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.502299Z",
     "iopub.status.busy": "2021-06-09T12:08:14.501741Z",
     "iopub.status.idle": "2021-06-09T12:08:14.504359Z",
     "shell.execute_reply": "2021-06-09T12:08:14.503896Z"
    },
    "id": "bttJ4uxZUQBr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the Cartpole environment:\r\n",
    "\r\n",
    "-   `observation` is an array of 4 floats: \r\n",
    "    -   the position and velocity of the cart\r\n",
    "    -   the angular position and velocity of the pole \r\n",
    "-   `reward` is a scalar float value\r\n",
    "-   `action` is a scalar integer with only two possible values:\r\n",
    "    -   `0` — \"move left\"\r\n",
    "    -   `1` — \"move right\"\r\n"
   ],
   "metadata": {
    "id": "eJCgJnx3g0yY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "time_step = env.reset()\r\n",
    "print('Time step:')\r\n",
    "print(time_step)\r\n",
    "\r\n",
    "action = np.array(1, dtype=np.int32)\r\n",
    "\r\n",
    "next_time_step = env.step(action)\r\n",
    "print('Next time step:')\r\n",
    "print(next_time_step)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time step:\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([[[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "       [[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]]], dtype=float32),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(0)})\n",
      "Next time step:\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([[[0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ]],\n",
      "\n",
      "       [[0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 1. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ]],\n",
      "\n",
      "       [[0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 1. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ]],\n",
      "\n",
      "       [[0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ]],\n",
      "\n",
      "       [[0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ]],\n",
      "\n",
      "       [[0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 1. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ]],\n",
      "\n",
      "       [[0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 1. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 1. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ]],\n",
      "\n",
      "       [[0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 1. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0.9, 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ]],\n",
      "\n",
      "       [[0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [1. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ]],\n",
      "\n",
      "       [[0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ],\n",
      "        [0. , 0. , 0. , 0. , 0. , 0. ]]], dtype=float32),\n",
      " 'reward': array(-2., dtype=float32),\n",
      " 'step_type': array(1)})\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.511263Z",
     "iopub.status.busy": "2021-06-09T12:08:14.510707Z",
     "iopub.status.idle": "2021-06-09T12:08:14.512918Z",
     "shell.execute_reply": "2021-06-09T12:08:14.513258Z"
    },
    "id": "V2UGR5t_iZX-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usually two environments are instantiated: one for training and one for evaluation. "
   ],
   "metadata": {
    "id": "4JSc9GviWUBK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "train_py_env = suite_gym.load(env_name)\r\n",
    "eval_py_env = suite_gym.load(env_name)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.518219Z",
     "iopub.status.busy": "2021-06-09T12:08:14.517279Z",
     "iopub.status.idle": "2021-06-09T12:08:14.520519Z",
     "shell.execute_reply": "2021-06-09T12:08:14.520079Z"
    },
    "id": "N7brXNIGWXjC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Cartpole environment, like most environments, is written in pure Python. This is converted to TensorFlow using the `TFPyEnvironment` wrapper.\r\n",
    "\r\n",
    "The original environment's API uses Numpy arrays. The `TFPyEnvironment` converts these to `Tensors` to make it compatible with Tensorflow agents and policies.\r\n"
   ],
   "metadata": {
    "id": "zuUqXAVmecTU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\r\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.524730Z",
     "iopub.status.busy": "2021-06-09T12:08:14.524161Z",
     "iopub.status.idle": "2021-06-09T12:08:14.528917Z",
     "shell.execute_reply": "2021-06-09T12:08:14.528525Z"
    },
    "id": "Xp-Y4mD6eDhF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agent\n",
    "\n",
    "The algorithm used to solve an RL problem is represented by an `Agent`. TF-Agents provides standard implementations of a variety of `Agents`, including:\n",
    "\n",
    "-   [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) (used in this tutorial)\n",
    "-   [REINFORCE](https://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n",
    "-   [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n",
    "-   [TD3](https://arxiv.org/pdf/1802.09477.pdf)\n",
    "-   [PPO](https://arxiv.org/abs/1707.06347)\n",
    "-   [SAC](https://arxiv.org/abs/1801.01290).\n",
    "\n",
    "The DQN agent can be used in any environment which has a discrete action space.\n",
    "\n",
    "At the heart of a DQN Agent is a `QNetwork`, a neural network model that can learn to predict `QValues` (expected returns) for all actions, given an observation from the environment.\n",
    "\n",
    "We will use `tf_agents.networks.` to create a `QNetwork`. The network will consist of a sequence of `tf.keras.layers.Dense` layers, where the final layer will have 1 output for each possible action."
   ],
   "metadata": {
    "id": "E9lW_OZYFR8A"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# fc_layer_params = (100, 50)\r\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\r\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\r\n",
    "\r\n",
    "# # Define a helper function to create Dense layers configured with the right\r\n",
    "# # activation and kernel initializer.\r\n",
    "# def dense_layer(num_units):\r\n",
    "#   return tf.keras.layers.Dense(\r\n",
    "#       num_units,\r\n",
    "#       activation=tf.keras.activations.relu,\r\n",
    "#       kernel_initializer=tf.keras.initializers.VarianceScaling(\r\n",
    "#           scale=2.0, mode='fan_in', distribution='truncated_normal'))\r\n",
    "\r\n",
    "# # QNetwork consists of a sequence of Dense layers followed by a dense layer\r\n",
    "# # with `num_actions` units to generate one q_value per available action as\r\n",
    "# # it's output.\r\n",
    "# dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\r\n",
    "# q_values_layer = tf.keras.layers.Dense(\r\n",
    "#     num_actions,\r\n",
    "#     activation=None,\r\n",
    "#     kernel_initializer=tf.keras.initializers.RandomUniform(\r\n",
    "#         minval=-0.03, maxval=0.03),\r\n",
    "#     bias_initializer=tf.keras.initializers.Constant(-0.2))\r\n",
    "# q_net = sequential.Sequential(dense_layers + [q_values_layer])\r\n",
    "\r\n",
    "from tensorflow.keras.models import Sequential, Model\r\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "\r\n",
    "def build_layers(state_size, action_size, learning_rate):\r\n",
    "    # Neural Net for Deep-Q learning Model\r\n",
    "    return [\r\n",
    "        Conv2D(filters=16,\r\n",
    "                kernel_size=(3,3),\r\n",
    "                padding='same',\r\n",
    "                # input_shape=state_size,\r\n",
    "                activation='elu',\r\n",
    "                kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal')),\r\n",
    "        Conv2D(filters=32,\r\n",
    "                kernel_size=(3,3),\r\n",
    "                padding='same',\r\n",
    "                # input_shape=state_size,\r\n",
    "                activation='elu',\r\n",
    "                kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal')),\r\n",
    "        MaxPool2D(),\r\n",
    "        Conv2D(filters=32,\r\n",
    "                kernel_size=(3,3),\r\n",
    "                padding='same',\r\n",
    "                activation='elu',\r\n",
    "                kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal')),\r\n",
    "        Flatten(),\r\n",
    "        Dense(256,\r\n",
    "            # input_dim=state_size,\r\n",
    "            activation='relu',\r\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal')),\r\n",
    "        Dense(64,\r\n",
    "            activation='relu',\r\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal')),\r\n",
    "        Dense(action_size,\r\n",
    "            activation='softmax',\r\n",
    "            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal'))\r\n",
    "    ]\r\n",
    "\r\n",
    "q_net = sequential.Sequential(build_layers(env.observation_shape, num_actions, learning_rate))"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.537481Z",
     "iopub.status.busy": "2021-06-09T12:08:14.534314Z",
     "iopub.status.idle": "2021-06-09T12:08:14.547005Z",
     "shell.execute_reply": "2021-06-09T12:08:14.546563Z"
    },
    "id": "TgkdEPg_muzV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now use `tf_agents.agents.dqn.dqn_agent` to instantiate a `DqnAgent`. In addition to the `time_step_spec`, `action_spec` and the QNetwork, the agent constructor also requires an optimizer (in this case, `AdamOptimizer`), a loss function, and an integer step counter."
   ],
   "metadata": {
    "id": "z62u55hSmviJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\r\n",
    "\r\n",
    "train_step_counter = tf.Variable(0)\r\n",
    "\r\n",
    "agent = dqn_agent.DqnAgent(\r\n",
    "    train_env.time_step_spec(),\r\n",
    "    train_env.action_spec(),\r\n",
    "    q_network=q_net,\r\n",
    "    optimizer=optimizer,\r\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\r\n",
    "    train_step_counter=train_step_counter)\r\n",
    "\r\n",
    "agent.initialize()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.557560Z",
     "iopub.status.busy": "2021-06-09T12:08:14.556642Z",
     "iopub.status.idle": "2021-06-09T12:08:14.583406Z",
     "shell.execute_reply": "2021-06-09T12:08:14.582980Z"
    },
    "id": "jbY4yrjTEyc9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policies\n",
    "\n",
    "A policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n",
    "\n",
    "In this tutorial:\n",
    "\n",
    "-   The desired outcome is keeping the pole balanced upright over the cart.\n",
    "-   The policy returns an action (left or right) for each `time_step` observation.\n",
    "\n",
    "Agents contain two policies: \n",
    "\n",
    "-   `agent.policy` — The main policy that is used for evaluation and deployment.\n",
    "-   `agent.collect_policy` — A second policy that is used for data collection.\n"
   ],
   "metadata": {
    "id": "I0KLrEPwkn5x"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "eval_policy = agent.policy\r\n",
    "collect_policy = agent.collect_policy"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.586763Z",
     "iopub.status.busy": "2021-06-09T12:08:14.586219Z",
     "iopub.status.idle": "2021-06-09T12:08:14.588277Z",
     "shell.execute_reply": "2021-06-09T12:08:14.587817Z"
    },
    "id": "BwY7StuMkuV4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Policies can be created independently of agents. For example, use `tf_agents.policies.random_tf_policy` to create a policy which will randomly select an action for each `time_step`."
   ],
   "metadata": {
    "id": "2Qs1Fl3dV0ae"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\r\n",
    "                                                train_env.action_spec())"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.591871Z",
     "iopub.status.busy": "2021-06-09T12:08:14.591259Z",
     "iopub.status.idle": "2021-06-09T12:08:14.593049Z",
     "shell.execute_reply": "2021-06-09T12:08:14.593386Z"
    },
    "id": "HE37-UCIrE69"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get an action from a policy, call the `policy.action(time_step)` method. The `time_step` contains the observation from the environment. This method returns a `PolicyStep`, which is a named tuple with three components:\n",
    "\n",
    "-   `action` — the action to be taken (in this case, `0` or `1`)\n",
    "-   `state` — used for stateful (that is, RNN-based) policies\n",
    "-   `info` — auxiliary data, such as log probabilities of actions"
   ],
   "metadata": {
    "id": "dOlnlRRsUbxP"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "example_environment = tf_py_environment.TFPyEnvironment(\r\n",
    "    suite_gym.load('gym_packman:Packman-v0'))"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.597963Z",
     "iopub.status.busy": "2021-06-09T12:08:14.597402Z",
     "iopub.status.idle": "2021-06-09T12:08:14.716890Z",
     "shell.execute_reply": "2021-06-09T12:08:14.717263Z"
    },
    "id": "5gCcpXswVAxk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "time_step = example_environment.reset()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.721349Z",
     "iopub.status.busy": "2021-06-09T12:08:14.720749Z",
     "iopub.status.idle": "2021-06-09T12:08:14.722701Z",
     "shell.execute_reply": "2021-06-09T12:08:14.723022Z"
    },
    "id": "D4DHZtq3Ndis"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "random_policy.action(time_step)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Received a mix of batched and unbatched Tensors, or Tensors are not compatible with Specs.  num_outer_dims: 1.\nSaw tensor_shapes:\n   TimeStep(\n{'discount': TensorShape([1]),\n 'observation': TensorShape([1, 4]),\n 'reward': TensorShape([1]),\n 'step_type': TensorShape([1])})\nAnd spec_shapes:\n   TimeStep(\n{'discount': TensorShape([]),\n 'observation': TensorShape([10, 10, 6]),\n 'reward': TensorShape([]),\n 'step_type': TensorShape([])})",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-6b48927ed1bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrandom_policy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tf_agents\\utils\\common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tf_agents\\policies\\random_tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    110\u001b[0m         self.observation_and_action_constraint_splitter)\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[0mouter_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_outer_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_time_step_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mobservation_and_action_constraint_splitter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m       observation, mask = observation_and_action_constraint_splitter(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tf_agents\\utils\\nest_utils.py\u001b[0m in \u001b[0;36mget_outer_shape\u001b[1;34m(nested_tensor, spec)\u001b[0m\n\u001b[0;32m    813\u001b[0m   \u001b[0mnum_outer_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m   if not is_batched_nested_tensors(\n\u001b[1;32m--> 815\u001b[1;33m       nested_tensor, spec, num_outer_dims=num_outer_dims, check_dtypes=False):\n\u001b[0m\u001b[0;32m    816\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tf_agents\\utils\\nest_utils.py\u001b[0m in \u001b[0;36mis_batched_nested_tensors\u001b[1;34m(tensors, specs, num_outer_dims, allow_extra_fields, check_dtypes)\u001b[0m\n\u001b[0;32m    533\u001b[0m       \u001b[1;34m'And spec_shapes:\\n   %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m       (num_outer_dims, tf.nest.pack_sequence_as(specs, tensor_shapes),\n\u001b[1;32m--> 535\u001b[1;33m        tf.nest.pack_sequence_as(specs, spec_shapes)))\n\u001b[0m\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Received a mix of batched and unbatched Tensors, or Tensors are not compatible with Specs.  num_outer_dims: 1.\nSaw tensor_shapes:\n   TimeStep(\n{'discount': TensorShape([1]),\n 'observation': TensorShape([1, 4]),\n 'reward': TensorShape([1]),\n 'step_type': TensorShape([1])})\nAnd spec_shapes:\n   TimeStep(\n{'discount': TensorShape([]),\n 'observation': TensorShape([10, 10, 6]),\n 'reward': TensorShape([]),\n 'step_type': TensorShape([])})"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.726419Z",
     "iopub.status.busy": "2021-06-09T12:08:14.725854Z",
     "iopub.status.idle": "2021-06-09T12:08:14.730052Z",
     "shell.execute_reply": "2021-06-09T12:08:14.730394Z"
    },
    "id": "PRFqAUzpNaAW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics and Evaluation\n",
    "\n",
    "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n",
    "\n",
    "The following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n"
   ],
   "metadata": {
    "id": "94rCXQtbUbXv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@test {\"skip\": true}\r\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\r\n",
    "\r\n",
    "  total_return = 0.0\r\n",
    "  for _ in range(num_episodes):\r\n",
    "\r\n",
    "    time_step = environment.reset()\r\n",
    "    episode_return = 0.0\r\n",
    "\r\n",
    "    while not time_step.is_last():\r\n",
    "      action_step = policy.action(time_step)\r\n",
    "      time_step = environment.step(action_step.action)\r\n",
    "      episode_return += time_step.reward\r\n",
    "    total_return += episode_return\r\n",
    "\r\n",
    "  avg_return = total_return / num_episodes\r\n",
    "  return avg_return.numpy()[0]\r\n",
    "\r\n",
    "\r\n",
    "# See also the metrics module for standard implementations of different metrics.\r\n",
    "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.735009Z",
     "iopub.status.busy": "2021-06-09T12:08:14.734447Z",
     "iopub.status.idle": "2021-06-09T12:08:14.736044Z",
     "shell.execute_reply": "2021-06-09T12:08:14.736380Z"
    },
    "id": "bitzHo5_UbXy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Running this computation on the `random_policy` shows a baseline performance in the environment."
   ],
   "metadata": {
    "id": "_snCVvq5Z8lJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:14.740249Z",
     "iopub.status.busy": "2021-06-09T12:08:14.739680Z",
     "iopub.status.idle": "2021-06-09T12:08:15.082571Z",
     "shell.execute_reply": "2021-06-09T12:08:15.082135Z"
    },
    "id": "9bgU6Q6BZ8Bp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The replay buffer keeps track of data collected from the environment. This tutorial uses `tf_agents.replay_buffers.tf_uniform_replay_buffer.TFUniformReplayBuffer`, as it is the most common. \n",
    "\n",
    "The constructor requires the specs for the data it will be collecting. This is available from the agent using the `collect_data_spec` method. The batch size and maximum buffer length are also required.\n"
   ],
   "metadata": {
    "id": "NLva6g2jdWgr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\r\n",
    "    data_spec=agent.collect_data_spec,\r\n",
    "    batch_size=train_env.batch_size,\r\n",
    "    max_length=replay_buffer_max_length)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.086440Z",
     "iopub.status.busy": "2021-06-09T12:08:15.085869Z",
     "iopub.status.idle": "2021-06-09T12:08:15.095564Z",
     "shell.execute_reply": "2021-06-09T12:08:15.095099Z"
    },
    "id": "vX2zGUWJGWAl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items."
   ],
   "metadata": {
    "id": "ZGNTDJpZs4NN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "agent.collect_data_spec"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.100519Z",
     "iopub.status.busy": "2021-06-09T12:08:15.099957Z",
     "iopub.status.idle": "2021-06-09T12:08:15.102769Z",
     "shell.execute_reply": "2021-06-09T12:08:15.102379Z"
    },
    "id": "_IZ-3HcqgE1z"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "agent.collect_data_spec._fields"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.106194Z",
     "iopub.status.busy": "2021-06-09T12:08:15.105617Z",
     "iopub.status.idle": "2021-06-09T12:08:15.108347Z",
     "shell.execute_reply": "2021-06-09T12:08:15.107944Z"
    },
    "id": "sy6g1tGcfRlw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Collection\n",
    "\n",
    "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer."
   ],
   "metadata": {
    "id": "rVD5nQ9ZGo8_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@test {\"skip\": true}\r\n",
    "def collect_step(environment, policy, buffer):\r\n",
    "  time_step = environment.current_time_step()\r\n",
    "  action_step = policy.action(time_step)\r\n",
    "  next_time_step = environment.step(action_step.action)\r\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\r\n",
    "\r\n",
    "  # Add trajectory to the replay buffer\r\n",
    "  buffer.add_batch(traj)\r\n",
    "\r\n",
    "def collect_data(env, policy, buffer, steps):\r\n",
    "  for _ in range(steps):\r\n",
    "    collect_step(env, policy, buffer)\r\n",
    "\r\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\r\n",
    "\r\n",
    "# This loop is so common in RL, that we provide standard implementations. \r\n",
    "# For more details see tutorial 4 or the drivers module.\r\n",
    "# https://github.com/tensorflow/agents/blob/master/docs/tutorials/4_drivers_tutorial.ipynb \r\n",
    "# https://www.tensorflow.org/agents/api_docs/python/tf_agents/drivers"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.113897Z",
     "iopub.status.busy": "2021-06-09T12:08:15.112829Z",
     "iopub.status.idle": "2021-06-09T12:08:15.327852Z",
     "shell.execute_reply": "2021-06-09T12:08:15.327409Z"
    },
    "id": "wr1KSAEGG4h9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The replay buffer is now a collection of Trajectories."
   ],
   "metadata": {
    "id": "84z5pQJdoKxo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# For the curious:\r\n",
    "# Uncomment to peel one of these off and inspect it.\r\n",
    "# iter(replay_buffer.as_dataset()).next()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.330914Z",
     "iopub.status.busy": "2021-06-09T12:08:15.330365Z",
     "iopub.status.idle": "2021-06-09T12:08:15.332044Z",
     "shell.execute_reply": "2021-06-09T12:08:15.332385Z"
    },
    "id": "4wZnLu2ViO4E"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
    "\n",
    "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
    "\n",
    "This dataset is also optimized by running parallel calls and prefetching data."
   ],
   "metadata": {
    "id": "TujU-PMUsKjS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\r\n",
    "dataset = replay_buffer.as_dataset(\r\n",
    "    num_parallel_calls=3, \r\n",
    "    sample_batch_size=batch_size, \r\n",
    "    num_steps=2).prefetch(3)\r\n",
    "\r\n",
    "\r\n",
    "dataset"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.335992Z",
     "iopub.status.busy": "2021-06-09T12:08:15.335415Z",
     "iopub.status.idle": "2021-06-09T12:08:15.917058Z",
     "shell.execute_reply": "2021-06-09T12:08:15.917395Z"
    },
    "id": "ba7bilizt_qW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "iterator = iter(dataset)\r\n",
    "print(iterator)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.920917Z",
     "iopub.status.busy": "2021-06-09T12:08:15.920327Z",
     "iopub.status.idle": "2021-06-09T12:08:15.978263Z",
     "shell.execute_reply": "2021-06-09T12:08:15.978640Z"
    },
    "id": "K13AST-2ppOq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# For the curious:\r\n",
    "# Uncomment to see what the dataset iterator is feeding to the agent.\r\n",
    "# Compare this representation of replay data \r\n",
    "# to the collection of individual trajectories shown earlier.\r\n",
    "\r\n",
    "# iterator.next()"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.981611Z",
     "iopub.status.busy": "2021-06-09T12:08:15.981053Z",
     "iopub.status.idle": "2021-06-09T12:08:15.982722Z",
     "shell.execute_reply": "2021-06-09T12:08:15.983047Z"
    },
    "id": "Th5w5Sff0b16"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the agent\n",
    "\n",
    "Two things must happen during the training loop:\n",
    "\n",
    "-   collect data from the environment\n",
    "-   use that data to train the agent's neural network(s)\n",
    "\n",
    "This example also periodicially evaluates the policy and prints the current score.\n",
    "\n",
    "The following will take ~5 minutes to run."
   ],
   "metadata": {
    "id": "hBc9lj9VWWtZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@test {\"skip\": true}\r\n",
    "try:\r\n",
    "  %%time\r\n",
    "except:\r\n",
    "  pass\r\n",
    "\r\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\r\n",
    "agent.train = common.function(agent.train)\r\n",
    "\r\n",
    "# Reset the train step\r\n",
    "agent.train_step_counter.assign(0)\r\n",
    "\r\n",
    "# Evaluate the agent's policy once before training.\r\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\r\n",
    "returns = [avg_return]\r\n",
    "\r\n",
    "for _ in range(num_iterations):\r\n",
    "\r\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\r\n",
    "    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\r\n",
    "\r\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\r\n",
    "    experience, unused_info = next(iterator)\r\n",
    "    train_loss = agent.train(experience).loss\r\n",
    "\r\n",
    "    step = agent.train_step_counter.numpy()\r\n",
    "\r\n",
    "    if step % log_interval == 0:\r\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\r\n",
    "\r\n",
    "    if step % eval_interval == 0:\r\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\r\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\r\n",
    "        returns.append(avg_return)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:08:15.990870Z",
     "iopub.status.busy": "2021-06-09T12:08:15.990298Z",
     "iopub.status.idle": "2021-06-09T12:12:24.897936Z",
     "shell.execute_reply": "2021-06-09T12:12:24.898275Z"
    },
    "id": "0pTbJ3PeyF-u"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization\n"
   ],
   "metadata": {
    "id": "68jNcA_TiJDq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Plots\r\n",
    "\r\n",
    "Use `matplotlib.pyplot` to chart how the policy improved during training.\r\n",
    "\r\n",
    "One iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)"
   ],
   "metadata": {
    "id": "aO-LWCdbbOIC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#@test {\"skip\": true}\r\n",
    "\r\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\r\n",
    "plt.plot(iterations, returns)\r\n",
    "plt.ylabel('Average Return')\r\n",
    "plt.xlabel('Iterations')\r\n",
    "plt.ylim(top=250)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:12:24.915349Z",
     "iopub.status.busy": "2021-06-09T12:12:24.914806Z",
     "iopub.status.idle": "2021-06-09T12:12:25.019504Z",
     "shell.execute_reply": "2021-06-09T12:12:25.019844Z"
    },
    "id": "NxtL1mbOYCVO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Videos"
   ],
   "metadata": {
    "id": "M7-XpPP99Cy7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Charts are nice. But more exciting is seeing an agent actually performing a task in an environment. \n",
    "\n",
    "First, create a function to embed videos in the notebook."
   ],
   "metadata": {
    "id": "9pGfGxSH32gn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def embed_mp4(filename):\r\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\r\n",
    "  video = open(filename,'rb').read()\r\n",
    "  b64 = base64.b64encode(video)\r\n",
    "  tag = '''\r\n",
    "  <video width=\"640\" height=\"480\" controls>\r\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\r\n",
    "  Your browser does not support the video tag.\r\n",
    "  </video>'''.format(b64.decode())\r\n",
    "\r\n",
    "  return IPython.display.HTML(tag)"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:12:25.024404Z",
     "iopub.status.busy": "2021-06-09T12:12:25.023843Z",
     "iopub.status.idle": "2021-06-09T12:12:25.025897Z",
     "shell.execute_reply": "2021-06-09T12:12:25.025498Z"
    },
    "id": "ULaGr8pvOKbl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python environment (the one \"inside\" the TensorFlow environment wrapper) provides a `render()` method, which outputs an image of the environment state. These can be collected into a video."
   ],
   "metadata": {
    "id": "9c_PH-pX4Pr5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\r\n",
    "  filename = filename + \".mp4\"\r\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\r\n",
    "    for _ in range(num_episodes):\r\n",
    "      time_step = eval_env.reset()\r\n",
    "      video.append_data(eval_py_env.render())\r\n",
    "      while not time_step.is_last():\r\n",
    "        action_step = policy.action(time_step)\r\n",
    "        time_step = eval_env.step(action_step.action)\r\n",
    "        video.append_data(eval_py_env.render())\r\n",
    "  return embed_mp4(filename)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "create_policy_eval_video(agent.policy, \"trained-agent\")"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:12:25.031028Z",
     "iopub.status.busy": "2021-06-09T12:12:25.030437Z",
     "iopub.status.idle": "2021-06-09T12:12:33.395239Z",
     "shell.execute_reply": "2021-06-09T12:12:33.395676Z"
    },
    "id": "owOVWB158NlF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For fun, compare the trained agent (above) to an agent moving randomly. (It does not do as well.)"
   ],
   "metadata": {
    "id": "povaAOcZygLw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "create_policy_eval_video(random_policy, \"random-agent\")"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-09T12:12:33.400440Z",
     "iopub.status.busy": "2021-06-09T12:12:33.399795Z",
     "iopub.status.idle": "2021-06-09T12:12:34.269219Z",
     "shell.execute_reply": "2021-06-09T12:12:34.269579Z"
    },
    "id": "pJZIdC37yNH4"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQN Tutorial.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "interpreter": {
   "hash": "e48d23369a553edc96f1373b6255b5687d82d68cd08867622b2500e338930542"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}