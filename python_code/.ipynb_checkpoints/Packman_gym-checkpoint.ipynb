{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build Packman Environment with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class PackmanEnv(Env):\n",
    "    rewards = {\n",
    "        'Start': 50,\n",
    "        0: -1,\n",
    "        1: -2,\n",
    "        2: -2,\n",
    "        3: -2,\n",
    "        4: -2,\n",
    "        'CollectDirt': 2,  # (-2 + 2 = 0)\n",
    "        'EndGame': 100\n",
    "    }\n",
    "    actions = {\n",
    "        0: \"Stay\",\n",
    "        1: \"Left\",\n",
    "        2: \"Up\",\n",
    "        3: \"Right\",\n",
    "        4: \"Down\"\n",
    "    }\n",
    "    def __init__(self):\n",
    "        # Actions we can take, left, down, stay, up, right\n",
    "        self.action_space = Discrete(5)\n",
    "        \n",
    "        # Define a 2-D observation space\n",
    "        self.observation_shape = (10, 10, 3)\n",
    "        self.observation_space = Box(low=np.zeros(self.observation_shape),\n",
    "                                            high=np.ones(self.observation_shape),\n",
    "                                            dtype=np.float32)\n",
    "        # Set start state\n",
    "        self.dict_state = self.init_state()\n",
    "        self.canvas = self.convertToImage(self.dict_state)\n",
    "        \n",
    "        # Load human model from the computer\n",
    "        self.human_model = tf.keras.models.load_model('./data/humanModel/mode_v0')\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0: \"Stay\"\n",
    "        # 1: \"Left\"\n",
    "        # 2: \"Up\"\n",
    "        # 3: \"Right\"\n",
    "        # 4: \"Down\"\n",
    "        computer_reward = 0\n",
    "        human_reward = 0\n",
    "        \n",
    "        # Assert that it is a valid action \n",
    "        assert self.action_space.contains(action), \"Invalid Action\"\n",
    "        \n",
    "        human_pos = np.where(self.dict_state['Human trace'] == 1)\n",
    "        computer_pos = np.where(self.dict_state['Computer trace'] == 1)\n",
    "        # predict next human action\n",
    "        \n",
    "        # when human model is ready uncomment this line\n",
    "#         human_action = self.predict_action(self.canvas)\n",
    "        human_action = self.get_random_valid_action(human_pos)\n",
    "    \n",
    "        assert self.valid_action(action, computer_pos) , \"Computer preformed invalid action: \" + str(action) + \" at pos: \" + str(computer_pos)\n",
    "        assert self.valid_action(human_action, human_pos) , \"Human preformed invalid action: \" + str(human_action) + \" at pos: \" + str(computer_pos)\n",
    "    \n",
    "        self.move(human_action, 'human') # assume human action is valid\n",
    "        # apply the action to the agent\n",
    "        self.move(action, 'computer')\n",
    "        \n",
    "        # check for clean dirt for both agents\n",
    "        dirts_pos = np.where(self.dict_state['All awards'] == 1)\n",
    "        for dirt_pos_i, dirt_pos_j in zip(dirts_pos[0], dirts_pos[1]):\n",
    "            if human_pos[0][0] == dirt_pos_i and human_pos[1][0] == dirt_pos_j:\n",
    "                self.dict_state['All awards'][human_pos] = 0\n",
    "                self.dict_state['Human awards'][human_pos] = 1\n",
    "                human_reward += self.rewards['CollectDirt']\n",
    "            if computer_pos[0][0] == dirt_pos_i and computer_pos[1][0] == dirt_pos_j:\n",
    "                self.dict_state['All awards'][computer_pos] = 0\n",
    "                self.dict_state['Computer awards'][computer_pos] = 1\n",
    "                computer_reward += self.rewards['CollectDirt']\n",
    "                            \n",
    "        # Reward for executing an action.\n",
    "        computer_reward = self.rewards[action]\n",
    "        human_reward = self.rewards[human_action]\n",
    "\n",
    "        if not np.any(self.dict_state['All awards']): # game ended when there is no dirt to clean\n",
    "            computer_reward += self.rewards['EndGame']\n",
    "            human_reward += self.rewards['EndGame']\n",
    "            done = True\n",
    "        else:\n",
    "            # Flag that marks the termination of an episode\n",
    "            done = False\n",
    "        \n",
    "        self.ep_return += computer_reward\n",
    "        self.ep_human_reward += human_reward\n",
    "        \n",
    "        self.canvas = self.convertToImage(self.dict_state)\n",
    "        \n",
    "        # Set placeholder for info\n",
    "        info = {\n",
    "                'done': done,\n",
    "               'current_reward': computer_reward,\n",
    "               'ep_return': self.ep_return,\n",
    "               'human_return':self.ep_human_reward,\n",
    "               }\n",
    "        \n",
    "        # Return step information\n",
    "        return self.canvas, computer_reward, done, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Implement viz\n",
    "        self.canvas = self.convertToImage(self.dict_state)\n",
    "        # Render the environment to the screen\n",
    "        if mode == 'human':\n",
    "            plt.figure(figsize=(9,9))\n",
    "            plt.imshow(np.asarray(self.canvas))\n",
    "            plt.axis('off')\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            info = {\n",
    "               'ep_return': self.ep_return,\n",
    "               'human_return':self.ep_human_reward,\n",
    "            }\n",
    "            plt.title(f'info: {info}')\n",
    "        elif mode == 'rgb_array':\n",
    "            return np.asarray(self.canvas)\n",
    "            \n",
    "    def reset(self):\n",
    "        # Reset the reward\n",
    "        self.ep_return = self.rewards['Start']\n",
    "        self.ep_human_reward = self.rewards['Start']\n",
    "        \n",
    "        # Reset board game\n",
    "        self.dict_state = self.init_state()\n",
    "        self.canvas = self.convertToImage(self.dict_state)\n",
    "        return self.canvas, self.ep_return\n",
    "    \n",
    "    def init_state(self):\n",
    "        # init board state with random n=5 dirt position\n",
    "        board = np.array([\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                    [0, 1, 1, 1, 1, 0, 1, 1, 1, 0],\n",
    "                    [0, 0, 1, 1, 0, 0, 0, 0, 1, 0],\n",
    "                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "                    [0, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
    "                    [0, 1, 0, 1, 0, 1, 0, 0, 1, 0],\n",
    "                    [0, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "                    [0, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n",
    "                    [0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n",
    "                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "                ])\n",
    "        human_trace = np.zeros(board.shape)\n",
    "        human_trace[2][2] = 1 # locate human player\n",
    "        board[2][2] = 0 # locate human player\n",
    "        \n",
    "        computer_trace = np.zeros(board.shape)\n",
    "        computer_trace[7][7] = 1 # locate computer player\n",
    "        board[7][7] = 0 # locate computer player\n",
    "        \n",
    "        human_awards = np.zeros(board.shape)\n",
    "        computer_awards = np.zeros(board.shape)\n",
    "\n",
    "        all_awards = np.zeros(board.shape)\n",
    "        idx = np.random.choice(np.count_nonzero(board), 5)\n",
    "        all_awards[tuple(map(lambda x: x[idx], np.where(board)))] = 1\n",
    "        board[2][2] = 1 # locate human player\n",
    "        board[7][7] = 1 # locate computer player\n",
    "        \n",
    "        return {\n",
    "                'Board': board,\n",
    "                'Human trace': human_trace,\n",
    "                'Computer trace': computer_trace,\n",
    "                'Human awards': human_awards,\n",
    "                'Computer awards': computer_awards,\n",
    "                'All awards': all_awards,\n",
    "                }\n",
    "    \n",
    "    def get_random_valid_action(self, pos):\n",
    "        random_action = self.action_space.sample()\n",
    "        while not self.valid_action(random_action, pos):\n",
    "            random_action = self.action_space.sample()\n",
    "        return random_action\n",
    "    \n",
    "    def valid_action(self, action, pos):\n",
    "        next_pos = self.new_pos(pos, action)\n",
    "        if self.dict_state['Board'][next_pos] == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def new_pos(self, current_pos, action):\n",
    "            if action == 0: # stay\n",
    "                return current_pos\n",
    "            elif action == 1: # left\n",
    "                return (current_pos[0], current_pos[1]-1)\n",
    "            elif action == 2: # up\n",
    "                return (current_pos[0]-1, current_pos[1])\n",
    "            elif action == 3: # right\n",
    "                return (current_pos[0], current_pos[1]+1)\n",
    "            elif action == 4: # down\n",
    "                return (current_pos[0]+1, current_pos[1])\n",
    "            else: # default case if action is not found\n",
    "                assert True , \"action: \" + str(action) + \" is not vallid at agent pos: \" + str(current_pos) + \" for agent: \" + str(agent) \n",
    "                \n",
    "    def move(self, action, agent):\n",
    "        # assume action is valid\n",
    "        if agent == 'human':\n",
    "            current_pos = np.where(self.dict_state['Human trace'] == 1)\n",
    "            self.dict_state['Human trace'] = self.dict_state['Human trace']*0.9\n",
    "            next_pos = self.new_pos(current_pos, action)\n",
    "            self.dict_state['Human trace'][next_pos] = 1\n",
    "        elif agent == 'computer':\n",
    "            current_pos = np.where(self.dict_state['Computer trace'] == 1)\n",
    "            self.dict_state['Computer trace'] = self.dict_state['Computer trace']*0.9\n",
    "            next_pos = self.new_pos(current_pos, action) \n",
    "            self.dict_state['Computer trace'][next_pos] = 1\n",
    "        else:\n",
    "            assert True , \"agent not define:\" + str(agent)\n",
    "    \n",
    "    def predict_action(self, img):\n",
    "        img_array = keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "        predictions = self.human_model.predict(img_array)\n",
    "        score = tf.nn.softmax(predictions[0])\n",
    "        \n",
    "        action = np.argmax(score)\n",
    "#         print(\n",
    "#             \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "#             .format(action, 100 * np.max(score))\n",
    "#         )\n",
    "        return action\n",
    "\n",
    "    def convertToImage(self, state):\n",
    "        r = state['Human awards']/2 + state['Human trace']\n",
    "        g = state['Board']/3 + state['All awards']\n",
    "        b = state['Computer awards']/2 + state['Computer trace']\n",
    "        rgb = np.dstack((r,g,b))\n",
    "        return (rgb - np.min(rgb)) / (np.max(rgb) - np.min(rgb)) # NormalizeImage\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PackmanEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state, score = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    \n",
    "    while not done:\n",
    "\n",
    "        computer_pos = np.where(env.dict_state['Computer trace'] == 1)\n",
    "        action = env.get_random_valid_action(computer_pos)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        score += reward\n",
    "#         print(info)\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = tensorflow.keras.models.Sequential()\n",
    "    model.add(Flatten(input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rl.agents import DQNAgent\n",
    "# from rl.policy import EpsGreedyQPolicy, LinearAnnealedPolicy\n",
    "# from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_agent(model, actions):\n",
    "#     policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=10000)\n",
    "#     memory = SequentialMemory(limit=50000, window_length=1)\n",
    "#     dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "#                   nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "#     return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=15, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Reloading Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
